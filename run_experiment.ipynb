{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dataset\n",
    "\n",
    "## Description\n",
    "\n",
    "## Pre-processing\n",
    "\n",
    "### Resizing\n",
    "\n",
    "### Random cropping\n",
    "\n",
    "### Corrupting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "batch_size = 128\n",
    "train_mean = [107.59252, 103.2752, 106.84143]\n",
    "train_std = [63.439133, 59.521027, 63.240288]\n",
    "# Preprocessing\n",
    "transform = T.Compose([\n",
    "                T.Normalize(train_mean, train_std)\n",
    "            ])\n",
    "\n",
    "train_set = TensorDataset(torch.load('train_x.pt'), torch.load('train_y.pt'))\n",
    "val_set = TensorDataset(torch.load('val_x.pt'), torch.load('val_y.pt'))\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "\n",
    "#train_rotate_set = TensorDataset(torch.load('train_x_rotate.pt'), torch.load('train_y_rotate.pt'))\n",
    "#val_rotate_set = TensorDataset(torch.load('val_x_rotate.pt'), torch.load('val_y_rotate.pt'))\n",
    "#test_rotate_set = TensorDataset(torch.load('test_x_rotate.pt'), torch.load('test_y_rotate.pt'))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#train_rotate_loader = DataLoader(train_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "#val_rotate_loader = DataLoader(val_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "#test_rotate_loader = DataLoader(test_rotate_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "source": [
    "# Training Procedures\n",
    "\n",
    "## Regular Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training pipelines\n",
    "def train_main(model, optimizer, loader_train, loader_val, epochs=1, model_path=None, early_stop_patience = 0):\n",
    "    \"\"\"\n",
    "    Train the main branch\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Logger object with loss and accuracy data\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    logger = Logger()\n",
    "    last_loss = float('inf')\n",
    "    for e in range(epochs):\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\", end='')\n",
    "\n",
    "        # Conclude Epoch\n",
    "        train_loss = loss.item()\n",
    "        train_acc = float(num_correct) / num_samples\n",
    "        val_loss, val_acc = evaluate_main(model, loader_val)\n",
    "        logger.log(train_loss, train_acc, val_loss, val_acc)\n",
    "        \n",
    "        # Early Stopping\n",
    "        if logger.check_early_stop(early_stop_patience):\n",
    "            print(\"[Early Stopped]\")\n",
    "            break\n",
    "        else:\n",
    "            if last_loss > val_loss:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss improved from %.4f to %.4f. Saving model to {model_path}.\".format(last_loss, val_loss))\n",
    "                if model_path is not None:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss did not improve from %.4f\".format(last_loss))\n",
    "            last_loss = val_loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_main(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return loss.item(), acc"
   ]
  },
  {
   "source": [
    "## Online Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt_online(model, loader, loader_spinned, optimizer):\n",
    "    \"\"\"\n",
    "    Online TTT with image spinning task\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    for x, y in loader_spinned:\n",
    "        x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        scores = model(x)\n",
    "        loss = F.cross_entropy(scores[1], y[1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return evaluate_main(model, loader)"
   ]
  },
  {
   "source": [
    "# Experiment 1: Baseline ResNet18"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/haoru/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "[Epoch 0, Batch 26] train_loss: 1.4036067724227905[Epoch 0] train_acc: 0.3261390887290168, val_acc:0.2577937649880096, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 1, Batch 26] train_loss: 2.7469780445098877[Epoch 1] train_acc: 0.46792565947242204, val_acc:0.4184652278177458, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 2, Batch 26] train_loss: 1.8481502532958984[Epoch 2] train_acc: 0.5314748201438849, val_acc:0.4172661870503597, val_loss did not improve from %.4f\n",
      "[Epoch 3, Batch 26] train_loss: 1.8004624843597412[Epoch 3] train_acc: 0.6025179856115108, val_acc:0.4904076738609113, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 4, Batch 26] train_loss: 2.1851603984832764[Epoch 4] train_acc: 0.6193045563549161, val_acc:0.5, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 5, Batch 26] train_loss: 1.8282390832901[Epoch 5] train_acc: 0.6888489208633094, val_acc:0.5935251798561151, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 6, Batch 26] train_loss: 1.5556771755218506[Epoch 6] train_acc: 0.7119304556354916, val_acc:0.5671462829736211, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 7, Batch 26] train_loss: 1.4967665672302246[Epoch 7] train_acc: 0.7344124700239808, val_acc:0.5875299760191847, val_loss did not improve from %.4f\n",
      "[Epoch 8, Batch 26] train_loss: 1.314589262008667[Epoch 8] train_acc: 0.7203237410071942, val_acc:0.6774580335731415, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 9, Batch 26] train_loss: 1.347299337387085[Epoch 9] train_acc: 0.7673860911270983, val_acc:0.6091127098321343, val_loss did not improve from %.4f\n",
      "[Epoch 10, Batch 26] train_loss: 0.8580142259597778[Epoch 10] train_acc: 0.7907673860911271, val_acc:0.7314148681055156, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 11, Batch 26] train_loss: 0.505641520023346[Epoch 11] train_acc: 0.8315347721822542, val_acc:0.8381294964028777, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 12, Batch 26] train_loss: 0.6338768601417542[Epoch 12] train_acc: 0.8779976019184652, val_acc:0.7805755395683454, val_loss did not improve from %.4f\n",
      "[Epoch 13, Batch 26] train_loss: 0.5872857570648193[Epoch 13] train_acc: 0.8729016786570744, val_acc:0.8369304556354916, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 14, Batch 26] train_loss: 1.337152123451233[Epoch 14] train_acc: 0.8899880095923262, val_acc:0.7829736211031175, val_loss did not improve from %.4f\n",
      "[Epoch 15, Batch 26] train_loss: 1.1841859817504883[Epoch 15] train_acc: 0.8708033573141487, val_acc:0.697841726618705, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 16, Batch 26] train_loss: 1.2150611877441406[Epoch 16] train_acc: 0.8519184652278178, val_acc:0.802158273381295, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 17, Batch 26] train_loss: 1.2176401615142822[Epoch 17] train_acc: 0.8779976019184652, val_acc:0.8189448441247003, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 18, Batch 26] train_loss: 0.543732762336731[Epoch 18] train_acc: 0.8654076738609112, val_acc:0.8321342925659473, val_loss did not improve from %.4f\n",
      "[Epoch 19, Batch 26] train_loss: 0.4121243357658386[Epoch 19] train_acc: 0.895083932853717, val_acc:0.8597122302158273, val_loss did not improve from %.4f\n",
      "[Epoch 20, Batch 26] train_loss: 0.46778231859207153[Epoch 20] train_acc: 0.9265587529976019, val_acc:0.9040767386091128, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 21, Batch 26] train_loss: 0.08700071275234222[Epoch 21] train_acc: 0.9475419664268585, val_acc:0.89568345323741, val_loss did not improve from %.4f\n",
      "[Epoch 22, Batch 26] train_loss: 0.7739585638046265[Epoch 22] train_acc: 0.9523381294964028, val_acc:0.89568345323741, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 23, Batch 26] train_loss: 1.0088210105895996[Epoch 23] train_acc: 0.9235611510791367, val_acc:0.86810551558753, val_loss did not improve from %.4f\n",
      "[Epoch 24, Batch 26] train_loss: 0.17143823206424713[Epoch 24] train_acc: 0.9310551558752997, val_acc:0.8908872901678657, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 25, Batch 26] train_loss: 0.8113083839416504[Epoch 25] train_acc: 0.947242206235012, val_acc:0.7362110311750599, val_loss did not improve from %.4f\n",
      "[Epoch 26, Batch 26] train_loss: 0.6759248375892639[Epoch 26] train_acc: 0.927757793764988, val_acc:0.86810551558753, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 27, Batch 26] train_loss: 0.5013806819915771[Epoch 27] train_acc: 0.9253597122302158, val_acc:0.854916067146283, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 28, Batch 26] train_loss: 0.4683605432510376[Epoch 28] train_acc: 0.9493405275779376, val_acc:0.9136690647482014, val_loss did not improve from %.4f\n",
      "[Epoch 29, Batch 26] train_loss: 0.17042189836502075[Epoch 29] train_acc: 0.9694244604316546, val_acc:0.9304556354916067, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 30, Batch 26] train_loss: 0.15337373316287994[Epoch 30] train_acc: 0.9754196642685852, val_acc:0.9220623501199041, val_loss did not improve from %.4f\n",
      "[Epoch 31, Batch 26] train_loss: 0.7052274942398071[Epoch 31] train_acc: 0.9742206235011991, val_acc:0.829736211031175, val_loss did not improve from %.4f\n",
      "[Epoch 32, Batch 26] train_loss: 0.7922436594963074[Epoch 32] train_acc: 0.9370503597122302, val_acc:0.7242206235011991, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 33, Batch 26] train_loss: 0.2100716531276703[Epoch 33] train_acc: 0.947242206235012, val_acc:0.8884892086330936, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 34, Batch 26] train_loss: 0.7468589544296265[Epoch 34] train_acc: 0.9784172661870504, val_acc:0.9172661870503597, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 35, Batch 26] train_loss: 0.17004452645778656[Epoch 35] train_acc: 0.9685251798561151, val_acc:0.9124700239808153, val_loss did not improve from %.4f\n",
      "[Epoch 36, Batch 26] train_loss: 0.12104246765375137[Epoch 36] train_acc: 0.9715227817745803, val_acc:0.935251798561151, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 37, Batch 26] train_loss: 0.4103146195411682[Epoch 37] train_acc: 0.980515587529976, val_acc:0.9124700239808153, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 38, Batch 26] train_loss: 0.02457575500011444[Epoch 38] train_acc: 0.9646282973621103, val_acc:0.9148681055155875, val_loss did not improve from %.4f\n",
      "[Epoch 39, Batch 26] train_loss: 0.8978763818740845[Epoch 39] train_acc: 0.9889088729016786, val_acc:0.8788968824940048, val_loss did not improve from %.4f\n",
      "[Epoch 40, Batch 26] train_loss: 0.09453509747982025[Epoch 40] train_acc: 0.9400479616306955, val_acc:0.9028776978417267, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 41, Batch 21] train_loss: 0.05918929725885391"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9011ba705a83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaselineResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m58\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-60d6064a431e>\u001b[0m in \u001b[0;36mtrain_main\u001b[0;34m(model, optimizer, loader_train, loader_val, epochs, model_path, early_stop_patience)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Conclude Epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment 1: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "from models import BaselineResNet\n",
    "model = BaselineResNet(58)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model, optimizer, train_loader, val_loader, epochs=100, model_path='model.pth', early_stop_patience=5)"
   ]
  },
  {
   "source": [
    "## Evaluation\n",
    "### Uncorrupted Images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/haoru/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(3.170003652572632, 0.522567703109328)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "from models import BaselineResNet\n",
    "model_path = 'model.pth'\n",
    "model = BaselineResNet(58)\n",
    "params = torch.load(model_path)\n",
    "model.load_state_dict(params)\n",
    "model = model.to(device=device)\n",
    "evaluate_main(model, test_loader)\n"
   ]
  },
  {
   "source": [
    "### Corrupted Images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Experiment 2: ResNet18 with Auxillary Branch (No Online Training)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Train a ResNet18 with auxillary branch"
   ]
  },
  {
   "source": [
    "# Experiment 3: ResNet18 with Auxillary Branch (Online-Trained)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Do online training on the auxillary branch, with pre-trained shared and main branch weights from experiment 1."
   ]
  }
 ]
}