{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Description\n",
    "\n",
    "## Pre-processing\n",
    "\n",
    "### Resizing\n",
    "\n",
    "### Random cropping\n",
    "\n",
    "### Corrupting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "batch_size = 128\n",
    "train_mean = [107.59252, 103.2752, 106.84143]\n",
    "train_std = [63.439133, 59.521027, 63.240288]\n",
    "# Preprocessing\n",
    "transform = T.Compose([\n",
    "                T.Normalize(train_mean, train_std)\n",
    "            ])\n",
    "\n",
    "train_set = TensorDataset(torch.load('train_x.pt'), torch.load('train_y.pt'))\n",
    "val_set = TensorDataset(torch.load('val_x.pt'), torch.load('val_y.pt'))\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "\n",
    "train_rotate_set = TensorDataset(torch.load('train_x_rotate.pt'), torch.load('train_y_rotate.pt'))\n",
    "val_rotate_set = TensorDataset(torch.load('val_x_rotate.pt'), torch.load('val_y_rotate.pt'))\n",
    "test_rotate_set = TensorDataset(torch.load('test_x_rotate.pt'), torch.load('test_y_rotate.pt'))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_rotate_loader = DataLoader(train_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "val_rotate_loader = DataLoader(val_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "test_rotate_loader = DataLoader(test_rotate_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Procedures\n",
    "\n",
    "## Regular Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training pipelines\n",
    "def train_main(model, optimizer, loader_train, loader_val, epochs=1, model_path=None, early_stop_patience = 0):\n",
    "    \"\"\"\n",
    "    Train the main branch\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Logger object with loss and accuracy data\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    logger = Logger()\n",
    "    last_loss = float('inf')\n",
    "    for e in range(epochs):\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        total_loss = 0.0\n",
    "        count = 0\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\", end='')\n",
    "            count += 1\n",
    "\n",
    "        # Conclude Epoch\n",
    "        train_loss = total_loss / count\n",
    "        train_acc = float(num_correct) / num_samples\n",
    "        val_loss, val_acc = evaluate_main(model, loader_val)\n",
    "        logger.log(train_loss, train_acc, val_loss, val_acc)\n",
    "        \n",
    "        with open(model_path.split('.')[0] + '.pkl', 'wb') as output_file:\n",
    "            pickle.dump(logger, output_file)\n",
    "\n",
    "        # Early Stopping\n",
    "        if logger.check_early_stop(early_stop_patience):\n",
    "            print(\"[Early Stopped]\")\n",
    "            break\n",
    "        else:\n",
    "            if last_loss > val_loss:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss improved from %.4f to %.4f. Saving model to {model_path}.\" % (last_loss, val_loss))\n",
    "                if model_path is not None:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss did not improve from %.4f\" % (last_loss))\n",
    "            last_loss = val_loss\n",
    "    return logger\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_both(model, optimizer, loader_train, loader_val, epochs=1, model_path=None, early_stop_patience = 0):\n",
    "    \"\"\"\n",
    "    Train the main and auxillary branch\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Logger object with loss and accuracy data\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    logger = Logger()\n",
    "    last_loss = float('inf')\n",
    "    for e in range(epochs):\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss_main = F.cross_entropy(scores[0], y[:, 0])\n",
    "            loss_auxillary = F.cross_entropy(scores[1], y[:, 1])\n",
    "            loss = loss_main + loss_auxillary\n",
    "            running_loss += loss_main.item()\n",
    "\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (preds == y[:, 0]).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\", end='')\n",
    "            count += 1\n",
    "\n",
    "        # Conclude Epoch\n",
    "        train_loss = running_loss / count\n",
    "        train_acc = float(num_correct) / num_samples\n",
    "        val_loss, val_acc = evaluate_both(model, loader_val)\n",
    "        logger.log(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "        with open(model_path.split('.')[0] + '.pkl', 'wb') as output_file:\n",
    "            pickle.dump(logger, output_file)\n",
    "            \n",
    "        # Early Stopping\n",
    "        if logger.check_early_stop(early_stop_patience):\n",
    "            print(\"[Early Stopped]\")\n",
    "            break\n",
    "        else:\n",
    "            if last_loss > val_loss:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss improved from %.4f to %.4f. Saving model to {model_path}.\" % (last_loss, val_loss))\n",
    "                if model_path is not None:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss did not improve from %.4f\" % (last_loss))\n",
    "            last_loss = val_loss\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_main(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    ave_loss = 0.0\n",
    "    count = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            ave_loss += loss.item()\n",
    "            count += 1\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return ave_loss / count, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_both(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy in model with two predictions\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    ave_loss = 0.0\n",
    "    count = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss_main = F.cross_entropy(scores[0], y[:, 0])\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (preds == y[:, 0]).sum()\n",
    "            # print(f\"num_correct: {num_correct}\")\n",
    "            num_samples += preds.size(0)\n",
    "            # print(f\"num_samples: {num_samples}\")\n",
    "            ave_loss += loss_main.item()\n",
    "            count += 1\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return ave_loss / count, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_non_rotate(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy in model with two predictions\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    from random import randrange\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    ave_loss = 0.0\n",
    "    count = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "#             random_number = randrange(4)\n",
    "#             if random_number == 0:\n",
    "#                 pass\n",
    "#             elif random_number == 1:\n",
    "#                 for i in range(x.size()[0]):\n",
    "#                     x[i][0] = torch.rot90(x[i][0])\n",
    "#                     x[i][1] = torch.rot90(x[i][1])\n",
    "#                     x[i][2] = torch.rot90(x[i][2])\n",
    "#             elif random_number == 2:\n",
    "#                 for i in range(x.size()[0]):\n",
    "#                     x[i][0] = torch.rot90(x[i][0], 2)\n",
    "#                     x[i][1] = torch.rot90(x[i][1], 2)\n",
    "#                     x[i][2] = torch.rot90(x[i][2], 2)\n",
    "#             elif random_number == 3:\n",
    "#                 for i in range(x.size()[0]):\n",
    "#                     x[i][0] = torch.rot90(x[i][0], 3)\n",
    "#                     x[i][1] = torch.rot90(x[i][1], 3)\n",
    "#                     x[i][2] = torch.rot90(x[i][2], 3)\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss_main = F.cross_entropy(scores[0], y)\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            #print(f\"num_correct: {num_correct}\")\n",
    "            num_samples += preds.size(0)\n",
    "            #print(f\"num_samples: {num_samples}\")\n",
    "            ave_loss += loss_main.item()\n",
    "            count += 1\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return ave_loss / count, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt_online(model, loader, loader_spinned, optimizer):\n",
    "    \"\"\"\n",
    "    Online TTT with image spinning task\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    for x, y in loader_spinned:\n",
    "        x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        scores = model(x)\n",
    "        loss_auxillary = F.cross_entropy(scores[1], y[:, 1])\n",
    "        optimizer.zero_grad()\n",
    "        loss_auxillary.backward()\n",
    "        optimizer.step()\n",
    "    return evaluate_main(model, loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Baseline ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.12248150981892375, val_acc:0.3569697742634868, val_loss improved from inf to 2.2720. Saving model to model_base_1.pth.\n",
      "[Epoch 1] train_acc: 0.6198355011476664, val_acc:0.7555158780767759, val_loss improved from 2.2720 to 0.8263. Saving model to model_base_1.pth.\n",
      "[Epoch 2] train_acc: 0.8285195103289977, val_acc:0.8477235046550184, val_loss improved from 0.8263 to 0.5327. Saving model to model_base_1.pth.\n",
      "[Epoch 3] train_acc: 0.8886444784493751, val_acc:0.884836117842112, val_loss improved from 0.5327 to 0.4087. Saving model to model_base_1.pth.\n",
      "[Epoch 4] train_acc: 0.9238714613618975, val_acc:0.8942736895804106, val_loss improved from 0.4087 to 0.3710. Saving model to model_base_1.pth.\n",
      "[Epoch 5] train_acc: 0.9361769956643713, val_acc:0.922203800535646, val_loss improved from 0.3710 to 0.2781. Saving model to model_base_1.pth.\n",
      "[Epoch 6] train_acc: 0.9510010201479214, val_acc:0.9234791480678485, val_loss improved from 0.2781 to 0.2671. Saving model to model_base_1.pth.\n",
      "[Epoch 7] train_acc: 0.961138740117317, val_acc:0.9233516133146282, val_loss did not improve from 0.2671\n",
      "[Epoch 8] train_acc: 0.962318286151492, val_acc:0.927305190664456, val_loss improved from 0.2869 to 0.2690. Saving model to model_base_1.pth.\n",
      "[Epoch 9] train_acc: 0.9696505993369039, val_acc:0.932789185052927, val_loss improved from 0.2690 to 0.2484. Saving model to model_base_1.pth.\n",
      "[Epoch 10] train_acc: 0.9769191532772252, val_acc:0.9307486290014029, val_loss did not improve from 0.2484\n",
      "[Epoch 11] train_acc: 0.9828168834480999, val_acc:0.9269225864047953, val_loss did not improve from 0.2612\n",
      "[Epoch 12] train_acc: 0.9821155317521041, val_acc:0.9480933554393572, val_loss improved from 0.2754 to 0.1953. Saving model to model_base_1.pth.\n",
      "[Epoch 13] train_acc: 0.9807447079826574, val_acc:0.9359775538834333, val_loss did not improve from 0.1953\n",
      "[Epoch 14] train_acc: 0.9850484570262689, val_acc:0.9341920673383497, val_loss did not improve from 0.2510\n",
      "[Epoch 15] train_acc: 0.9856222902320837, val_acc:0.9443948475959699, val_loss improved from 0.2590 to 0.2399. Saving model to model_base_1.pth.\n",
      "[Epoch 16] train_acc: 0.986259882682989, val_acc:0.9399311312332611, val_loss did not improve from 0.2399\n",
      "[Epoch 17] train_acc: 0.9894159653149707, val_acc:0.9395485269736003, val_loss did not improve from 0.2426\n",
      "[Epoch 18] train_acc: 0.981637337413925, val_acc:0.938910853207499, val_loss improved from 0.2651 to 0.2575. Saving model to model_base_1.pth.\n",
      "[Epoch 19] train_acc: 0.9880770211680694, val_acc:0.9438847085830889, val_loss improved from 0.2575 to 0.2490. Saving model to model_base_1.pth.\n",
      "[Epoch 20] train_acc: 0.9899897985207855, val_acc:0.9454151256217319, val_loss improved from 0.2490 to 0.2349. Saving model to model_base_1.pth.\n",
      "[Epoch 21] train_acc: 0.9938153532262178, val_acc:0.9307486290014029, val_loss did not improve from 0.2349\n",
      "[Epoch 22] train_acc: 0.9898304004080591, val_acc:0.9472006121668155, val_loss improved from 0.2999 to 0.2449. Saving model to model_base_1.pth.\n",
      "[Epoch 23] train_acc: 0.993241520020403, val_acc:0.9445223823491902, val_loss did not improve from 0.2449\n",
      "[Epoch 24] train_acc: 0.9894797245600612, val_acc:0.9487310292054585, val_loss improved from 0.2627 to 0.2421. Saving model to model_base_1.pth.\n",
      "[Epoch 25] train_acc: 0.9907230298393267, val_acc:0.9483484249457977, val_loss improved from 0.2421 to 0.2416. Saving model to model_base_1.pth.\n",
      "[Epoch 26] train_acc: 0.9912968630451415, val_acc:0.9341920673383497, val_loss did not improve from 0.2416\n",
      "[Epoch 27] train_acc: 0.9901810762560571, val_acc:0.9341920673383497, val_loss improved from 0.3036 to 0.2941. Saving model to model_base_1.pth.\n",
      "[Epoch 28] train_acc: 0.9902129558786024, val_acc:0.9472006121668155, val_loss improved from 0.2941 to 0.2269. Saving model to model_base_1.pth.\n",
      "[Epoch 29] train_acc: 0.9903085947462382, val_acc:0.9463078688942737, val_loss did not improve from 0.2269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<logger.Logger at 0x25be5ab5100>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "from models import BaselineResNet\n",
    "model_base_1 = BaselineResNet(43)\n",
    "optimizer = optim.Adam(model_base_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_base_1, optimizer, train_loader, val_loader, epochs=30, model_path='model_base_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Uncorrupted Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.43273189961910247, 0.9043547110055423)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import BaselineResNet\n",
    "model_path = 'model_base_1.pth'\n",
    "model_base_1 = BaselineResNet(43)\n",
    "params = torch.load(model_path)\n",
    "model_base_1.load_state_dict(params)\n",
    "model_base_1 = model_base_1.to(device=device)\n",
    "evaluate_main(model_base_1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1.5: Baseline CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.4020976791634787, val_acc:0.6983803086341028, val_loss improved from inf to 1.0278. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 1] train_acc: 0.8483167559296098, val_acc:0.8686392041831399, val_loss improved from 1.0278 to 0.4386. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 2] train_acc: 0.927250701351696, val_acc:0.907537303915317, val_loss improved from 0.4386 to 0.3340. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 3] train_acc: 0.9595766386125988, val_acc:0.9270501211580155, val_loss improved from 0.3340 to 0.2903. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 4] train_acc: 0.9738587095128793, val_acc:0.9296008162224206, val_loss did not improve from 0.2903\n",
      "[Epoch 5] train_acc: 0.9751657740372354, val_acc:0.9164647366407346, val_loss did not improve from 0.3384\n",
      "[Epoch 6] train_acc: 0.9775567457281306, val_acc:0.9380181099349573, val_loss improved from 0.3908 to 0.2848. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 7] train_acc: 0.9848890589135425, val_acc:0.936870297155975, val_loss did not improve from 0.2848\n",
      "[Epoch 8] train_acc: 0.9849209385360878, val_acc:0.938400714194618, val_loss did not improve from 0.3047\n",
      "[Epoch 9] train_acc: 0.9852078551389951, val_acc:0.9308761637546231, val_loss did not improve from 0.3289\n",
      "[Epoch 10] train_acc: 0.985303494006631, val_acc:0.9287080729498789, val_loss improved from 0.3562 to 0.3414. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 11] train_acc: 0.9865786789084418, val_acc:0.9441397780895294, val_loss improved from 0.3414 to 0.3043. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 12] train_acc: 0.9882364192807958, val_acc:0.9496237724780002, val_loss improved from 0.3043 to 0.2742. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 13] train_acc: 0.9929546034174955, val_acc:0.9400586659864814, val_loss did not improve from 0.2742\n",
      "[Epoch 14] train_acc: 0.9854628921193573, val_acc:0.9459252646346129, val_loss improved from 0.3621 to 0.2943. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 15] train_acc: 0.9888102524866106, val_acc:0.9470730774135953, val_loss did not improve from 0.2943\n",
      "[Epoch 16] train_acc: 0.9899579188982403, val_acc:0.9429919653105471, val_loss did not improve from 0.3051\n",
      "[Epoch 17] train_acc: 0.9920300943636827, val_acc:0.9473281469200358, val_loss improved from 0.3098 to 0.2800. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 18] train_acc: 0.9926995664371334, val_acc:0.9340645325851294, val_loss did not improve from 0.2800\n",
      "[Epoch 19] train_acc: 0.988618974751339, val_acc:0.9470730774135953, val_loss improved from 0.3917 to 0.2674. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 20] train_acc: 0.992061973986228, val_acc:0.9455426603749522, val_loss did not improve from 0.2674\n",
      "[Epoch 21] train_acc: 0.9914881407804131, val_acc:0.9492411682183395, val_loss improved from 0.2988 to 0.2788. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 22] train_acc: 0.9931140015302219, val_acc:0.9417166177783446, val_loss did not improve from 0.2788\n",
      "[Epoch 23] train_acc: 0.9913287426676868, val_acc:0.9493687029715597, val_loss improved from 0.3605 to 0.2722. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 24] train_acc: 0.9933052792654935, val_acc:0.9435021043234282, val_loss did not improve from 0.2722\n",
      "[Epoch 25] train_acc: 0.9895434838051518, val_acc:0.9418441525315648, val_loss improved from 0.3522 to 0.2924. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 26] train_acc: 0.9927952053047692, val_acc:0.9456701951281724, val_loss did not improve from 0.2924\n",
      "[Epoch 27] train_acc: 0.9911055853098699, val_acc:0.953577349827828, val_loss improved from 0.3265 to 0.2635. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 28] train_acc: 0.9956324917112981, val_acc:0.9477107511796965, val_loss did not improve from 0.2635\n",
      "[Epoch 29] train_acc: 0.9928589645498598, val_acc:0.937890575181737, val_loss did not improve from 0.3281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<logger.Logger at 0x25b0f00e0d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1: Train a baseline CNN: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "from cnn_models import BaselineCNN\n",
    "cnn_model_base_1 = BaselineCNN(43)\n",
    "optimizer = optim.Adam(cnn_model_base_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(cnn_model_base_1, optimizer, train_loader, val_loader, epochs=30, model_path='cnn_model_base_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Uncorrupted Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4685808004754962, 0.9095011876484561)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cnn_models import BaselineCNN\n",
    "model_path = 'cnn_model_base_1.pth'\n",
    "cnn_model_base_1 = BaselineCNN(43)\n",
    "params = torch.load(model_path)\n",
    "cnn_model_base_1.load_state_dict(params)\n",
    "cnn_model_base_1 = cnn_model_base_1.to(device=device)\n",
    "evaluate_main(cnn_model_base_1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: ResNet18 with Auxillary Branch (No Online Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.16449885233358838, val_acc:0.28650682310929726, val_loss improved from inf to 2.4156. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 1] train_acc: 0.4454699056363173, val_acc:0.49295370488458107, val_loss improved from 2.4156 to 1.6854. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 2] train_acc: 0.6814986610558531, val_acc:0.7458232368320368, val_loss improved from 1.6854 to 0.8470. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 3] train_acc: 0.851361259882683, val_acc:0.891914296645836, val_loss improved from 0.8470 to 0.3726. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 4] train_acc: 0.9133352461106861, val_acc:0.9018620073970157, val_loss improved from 0.3726 to 0.3400. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 5] train_acc: 0.9386237566947208, val_acc:0.9341601836500446, val_loss improved from 0.3400 to 0.2309. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 6] train_acc: 0.953049285896455, val_acc:0.9518237469710497, val_loss improved from 0.2309 to 0.1681. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 7] train_acc: 0.9614256567202244, val_acc:0.9557773243208775, val_loss improved from 0.1681 to 0.1568. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 8] train_acc: 0.9663351185921959, val_acc:0.9545976278535901, val_loss improved from 0.1568 to 0.1541. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 9] train_acc: 0.9711887911247131, val_acc:0.9603685754368065, val_loss improved from 0.1541 to 0.1378. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 10] train_acc: 0.9755881790359602, val_acc:0.9617076903456192, val_loss did not improve from 0.1378\n",
      "[Epoch 11] train_acc: 0.9786565927059424, val_acc:0.9613250860859585, val_loss did not improve from 0.1411\n",
      "[Epoch 12] train_acc: 0.981159143075746, val_acc:0.9635569442673129, val_loss improved from 0.1425 to 0.1411. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 13] train_acc: 0.9828806426931905, val_acc:0.968180079071547, val_loss improved from 0.1411 to 0.1220. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 14] train_acc: 0.9840522188217291, val_acc:0.9694235429154444, val_loss did not improve from 0.1220\n",
      "[Epoch 15] train_acc: 0.9862120632491711, val_acc:0.9707307741359521, val_loss improved from 0.1261 to 0.1237. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 16] train_acc: 0.9882125095638867, val_acc:0.9701568677464609, val_loss did not improve from 0.1237\n",
      "[Epoch 17] train_acc: 0.9888501020147922, val_acc:0.9677974748118863, val_loss did not improve from 0.1309\n",
      "[Epoch 18] train_acc: 0.989575363427697, val_acc:0.9682119627598521, val_loss did not improve from 0.1465\n",
      "[Epoch 19] train_acc: 0.9903165646518746, val_acc:0.9698699145517153, val_loss improved from 0.1482 to 0.1272. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 20] train_acc: 0.9924524993624075, val_acc:0.9690090549674787, val_loss did not improve from 0.1272\n",
      "[Epoch 21] train_acc: 0.9924206197398623, val_acc:0.96971049611019, val_loss improved from 0.1389 to 0.1341. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 22] train_acc: 0.9930582121907676, val_acc:0.9666177783445989, val_loss did not improve from 0.1341\n",
      "[Epoch 23] train_acc: 0.9933052792654935, val_acc:0.9706670067593419, val_loss improved from 0.1641 to 0.1427. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 24] train_acc: 0.993169790869676, val_acc:0.9659163372018875, val_loss did not improve from 0.1427\n",
      "[Epoch 25] train_acc: 0.9942536980362152, val_acc:0.9720061216681546, val_loss improved from 0.1555 to 0.1398. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 26] train_acc: 0.9943094873756695, val_acc:0.9716872847851039, val_loss did not improve from 0.1398\n",
      "[Epoch 27] train_acc: 0.9940863300178526, val_acc:0.9729307486290014, val_loss improved from 0.1462 to 0.1357. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 28] train_acc: 0.9948434710533027, val_acc:0.9712727968371381, val_loss did not improve from 0.1357\n",
      "[Epoch 29] train_acc: 0.9948912904871207, val_acc:0.9719104706032394, val_loss improved from 0.1452 to 0.1353. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 30] train_acc: 0.9946123437898495, val_acc:0.9727713301874761, val_loss did not improve from 0.1353\n",
      "[Epoch 31] train_acc: 0.9951702371843917, val_acc:0.972739446499171, val_loss improved from 0.1446 to 0.1367. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 32] train_acc: 0.9954013644478449, val_acc:0.9724206096161204, val_loss improved from 0.1367 to 0.1329. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 33] train_acc: 0.9952818158633002, val_acc:0.9760872337712027, val_loss improved from 0.1329 to 0.1218. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 34] train_acc: 0.9959353481254782, val_acc:0.9736959571483229, val_loss did not improve from 0.1218\n",
      "[Epoch 35] train_acc: 0.9954651236929355, val_acc:0.9748437699273051, val_loss improved from 0.1400 to 0.1344. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 36] train_acc: 0.9959034685029329, val_acc:0.9717510521617141, val_loss did not improve from 0.1344\n",
      "[Epoch 37] train_acc: 0.9960389568987503, val_acc:0.9729945160056115, val_loss improved from 0.1407 to 0.1399. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 38] train_acc: 0.9952658760520275, val_acc:0.9692960081622242, val_loss did not improve from 0.1399\n",
      "[Epoch 39] train_acc: 0.9963497832185667, val_acc:0.9733771202652722, val_loss improved from 0.1488 to 0.1434. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 40] train_acc: 0.996333843407294, val_acc:0.9735046550184926, val_loss improved from 0.1434 to 0.1374. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 41] train_acc: 0.996381662841112, val_acc:0.9726437954342558, val_loss did not improve from 0.1374\n",
      "[Epoch 42] train_acc: 0.9960070772762051, val_acc:0.9760872337712027, val_loss improved from 0.1439 to 0.1320. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 43] train_acc: 0.9962621142565672, val_acc:0.9763104195893381, val_loss improved from 0.1320 to 0.1243. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 44] train_acc: 0.9971627135934711, val_acc:0.9703481698762912, val_loss did not improve from 0.1243\n",
      "[Epoch 45] train_acc: 0.9963896327467483, val_acc:0.9727713301874761, val_loss improved from 0.1562 to 0.1376. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 46] train_acc: 0.9965888803876563, val_acc:0.9743973982910343, val_loss did not improve from 0.1376\n",
      "[Epoch 47] train_acc: 0.9966366998214741, val_acc:0.9714322152786634, val_loss improved from 0.1439 to 0.1383. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 48] train_acc: 0.9968439173680184, val_acc:0.9726437954342558, val_loss improved from 0.1383 to 0.1334. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 49] train_acc: 0.9973221117061974, val_acc:0.9738234919015432, val_loss did not improve from 0.1334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<logger.Logger at 0x1d7ce149190>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1: Train a ResNet18 with auxillary branch\n",
    "lr = 5e-4\n",
    "wd = 1e-5\n",
    "from models import ResNetTwoBranch\n",
    "exp_2_model_1 = ResNetTwoBranch()\n",
    "optimizer = optim.Adam(exp_2_model_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_both(exp_2_model_1, optimizer, train_rotate_loader, val_rotate_loader, epochs=50, model_path='exp_2_model_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39871874469556384, 0.9288796516231196)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import ResNetTwoBranch\n",
    "model_path = 'exp_2_model_1.pth'\n",
    "exp_2_model_1 = ResNetTwoBranch()\n",
    "params = torch.load(model_path)\n",
    "exp_2_model_1.load_state_dict(params)\n",
    "exp_2_model_1 = exp_2_model_1.to(device=device)\n",
    "evaluate_both(exp_2_model_1, test_rotate_loader)\n",
    "#evaluate_non_rotate(exp_2_model_1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2.5: CNN with Auxillary Branch (No Online Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.054641673042591175, val_acc:0.061599285805381966, val_loss improved from inf to 3.4921. Saving model to exp_2_cnn_model_1.pth.\n",
      "[Epoch 1] train_acc: 0.05527129558786024, val_acc:0.060068868766738934, val_loss did not improve from 3.4921\n",
      "[Epoch 2, Batch 818] train_loss: 4.8913483619689945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3c03c75c5720>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mexp_2_cnn_model_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNNTwoBranch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_2_cnn_model_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_both\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_2_cnn_model_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_rotate_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_rotate_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'exp_2_cnn_model_1.pth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-4d88fca17904>\u001b[0m in \u001b[0;36mtrain_both\u001b[1;34m(model, optimizer, loader_train, loader_val, epochs, model_path, early_stop_patience)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment 1: Train a CNN with auxillary branch\n",
    "lr = 5e-4\n",
    "wd = 1e-5\n",
    "from cnn_models import CNNTwoBranch\n",
    "exp_2_cnn_model_1 = CNNTwoBranch()\n",
    "optimizer = optim.Adam(exp_2_cnn_model_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_both(exp_2_cnn_model_1, optimizer, train_rotate_loader, val_rotate_loader, epochs=50, model_path='exp_2_cnn_model_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: ResNet18 with Auxillary Branch (Online-Trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Do online training on the auxillary branch, with pre-trained shared and main branch weights from experiment 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
