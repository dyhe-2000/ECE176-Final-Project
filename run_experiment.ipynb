{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Description\n",
    "\n",
    "## Pre-processing\n",
    "\n",
    "### Resizing\n",
    "\n",
    "### Random cropping\n",
    "\n",
    "### Corrupting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "batch_size = 128\n",
    "train_mean = [107.59252, 103.2752, 106.84143]\n",
    "train_std = [63.439133, 59.521027, 63.240288]\n",
    "# Preprocessing\n",
    "transform = T.Compose([\n",
    "                T.Normalize(train_mean, train_std)\n",
    "            ])\n",
    "\n",
    "train_set = TensorDataset(torch.load('train_x.pt'), torch.load('train_y.pt'))\n",
    "val_set = TensorDataset(torch.load('val_x.pt'), torch.load('val_y.pt'))\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "\n",
    "#train_rotate_set = TensorDataset(torch.load('train_x_rotate.pt'), torch.load('train_y_rotate.pt'))\n",
    "#val_rotate_set = TensorDataset(torch.load('val_x_rotate.pt'), torch.load('val_y_rotate.pt'))\n",
    "#test_rotate_set = TensorDataset(torch.load('test_x_rotate.pt'), torch.load('test_y_rotate.pt'))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#train_rotate_loader = DataLoader(train_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "#val_rotate_loader = DataLoader(val_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "#test_rotate_loader = DataLoader(test_rotate_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Procedures\n",
    "\n",
    "## Regular Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training pipelines\n",
    "def train_main(model, optimizer, loader_train, loader_val, epochs=1, model_path=None, early_stop_patience = 0):\n",
    "    \"\"\"\n",
    "    Train the main branch\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Logger object with loss and accuracy data\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    logger = Logger()\n",
    "    last_loss = float('inf')\n",
    "    for e in range(epochs):\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\", end='')\n",
    "\n",
    "        # Conclude Epoch\n",
    "        train_loss = loss.item()\n",
    "        train_acc = float(num_correct) / num_samples\n",
    "        val_loss, val_acc = evaluate_main(model, loader_val)\n",
    "        logger.log(train_loss, train_acc, val_loss, val_acc)\n",
    "        \n",
    "        # Early Stopping\n",
    "        if logger.check_early_stop(early_stop_patience):\n",
    "            print(\"[Early Stopped]\")\n",
    "            break\n",
    "        else:\n",
    "            if last_loss > val_loss:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss improved from %.4f to %.4f. Saving model to {model_path}.\" % (last_loss, val_loss))\n",
    "                if model_path is not None:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss did not improve from %.4f\" % (last_loss))\n",
    "            last_loss = val_loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_main(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return loss.item(), acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt_online(model, loader, loader_spinned, optimizer):\n",
    "    \"\"\"\n",
    "    Online TTT with image spinning task\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    for x, y in loader_spinned:\n",
    "        x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        scores = model(x)\n",
    "        loss = F.cross_entropy(scores[1], y[1])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return evaluate_main(model, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Baseline ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/archive/v0.6.0.zip\" to C:\\Users\\hycme/.cache\\torch\\hub\\v0.6.0.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.4073741007194245, val_acc:0.420863309352518, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 1] train_acc: 0.595173860911271, val_acc:0.579136690647482, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 2] train_acc: 0.7150779376498801, val_acc:0.6672661870503597, val_loss did not improve from %.4f\n",
      "[Epoch 3] train_acc: 0.8204436450839329, val_acc:0.6936450839328537, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 4] train_acc: 0.8658573141486811, val_acc:0.8651079136690647, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 5] train_acc: 0.9063249400479616, val_acc:0.8489208633093526, val_loss did not improve from %.4f\n",
      "[Epoch 6] train_acc: 0.9349520383693045, val_acc:0.7649880095923262, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 7] train_acc: 0.9190647482014388, val_acc:0.8237410071942446, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 8] train_acc: 0.9484412470023981, val_acc:0.9322541966426858, val_loss did not improve from %.4f\n",
      "[Epoch 9] train_acc: 0.9602817745803357, val_acc:0.8471223021582733, val_loss did not improve from %.4f\n",
      "[Epoch 10] train_acc: 0.9496402877697842, val_acc:0.9232613908872902, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 11] train_acc: 0.9704736211031175, val_acc:0.9322541966426858, val_loss did not improve from %.4f\n",
      "[Epoch 12] train_acc: 0.9625299760191847, val_acc:0.7356115107913669, val_loss did not improve from %.4f\n",
      "[Epoch 13] train_acc: 0.9401978417266187, val_acc:0.9634292565947242, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 14] train_acc: 0.9895083932853717, val_acc:0.960431654676259, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 15] train_acc: 0.9907074340527577, val_acc:0.959832134292566, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 16] train_acc: 0.9913069544364509, val_acc:0.9244604316546763, val_loss did not improve from %.4f\n",
      "[Epoch 17] train_acc: 0.9647781774580336, val_acc:0.9424460431654677, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 18] train_acc: 0.9901079136690647, val_acc:0.9748201438848921, val_loss did not improve from %.4f\n",
      "[Epoch 19] train_acc: 0.9907074340527577, val_acc:0.9814148681055156, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 20] train_acc: 0.9985011990407674, val_acc:0.9856115107913669, val_loss did not improve from %.4f\n",
      "[Epoch 21] train_acc: 0.9961031175059952, val_acc:0.9808153477218226, val_loss improved from %.4f to %.4f. Saving model to model.pth.\n",
      "[Epoch 22, Batch 13] train_loss: 0.010127745568752289"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5cde176c4120>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaselineResNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m58\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model.pth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-e6f720b0e8e2>\u001b[0m in \u001b[0;36mtrain_main\u001b[1;34m(model, optimizer, loader_train, loader_val, epochs, model_path, early_stop_patience)\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# Conclude Epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment 1: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "from models import BaselineResNet\n",
    "model = BaselineResNet(58)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model, optimizer, train_loader, val_loader, epochs=50, model_path='model.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.3962829736211031, val_acc:0.4592326139088729, val_loss improved from inf to 1.2396. Saving model to model_base_1.pth.\n",
      "[Epoch 1] train_acc: 0.6312949640287769, val_acc:0.5821342925659473, val_loss improved from 1.2396 to 1.0678. Saving model to model_base_1.pth.\n",
      "[Epoch 2] train_acc: 0.7712829736211031, val_acc:0.6600719424460432, val_loss improved from 1.0678 to 0.1275. Saving model to model_base_1.pth.\n",
      "[Epoch 3] train_acc: 0.8324340527577938, val_acc:0.7068345323741008, val_loss did not improve from 0.1275\n",
      "[Epoch 4] train_acc: 0.8776978417266187, val_acc:0.8249400479616307, val_loss improved from 1.9390 to 0.0884. Saving model to model_base_1.pth.\n",
      "[Epoch 5] train_acc: 0.8995803357314148, val_acc:0.7799760191846523, val_loss did not improve from 0.0884\n",
      "[Epoch 6] train_acc: 0.9246103117505995, val_acc:0.8309352517985612, val_loss did not improve from 0.3738\n",
      "[Epoch 7] train_acc: 0.9279076738609112, val_acc:0.8189448441247003, val_loss improved from 0.6398 to 0.1083. Saving model to model_base_1.pth.\n",
      "[Epoch 8] train_acc: 0.9404976019184652, val_acc:0.8920863309352518, val_loss improved from 0.1083 to 0.0631. Saving model to model_base_1.pth.\n",
      "[Epoch 9] train_acc: 0.9413968824940048, val_acc:0.8866906474820144, val_loss improved from 0.0631 to 0.0278. Saving model to model_base_1.pth.\n",
      "[Epoch 10] train_acc: 0.9634292565947242, val_acc:0.8878896882494005, val_loss did not improve from 0.0278\n",
      "[Epoch 11] train_acc: 0.9716726618705036, val_acc:0.9568345323741008, val_loss improved from 1.0532 to 0.3391. Saving model to model_base_1.pth.\n",
      "[Epoch 12] train_acc: 0.982763788968825, val_acc:0.9622302158273381, val_loss improved from 0.3391 to 0.0097. Saving model to model_base_1.pth.\n",
      "[Epoch 13] train_acc: 0.9875599520383693, val_acc:0.9574340527577938, val_loss did not improve from 0.0097\n",
      "[Epoch 14] train_acc: 0.9866606714628298, val_acc:0.9382494004796164, val_loss improved from 0.4532 to 0.0275. Saving model to model_base_1.pth.\n",
      "[Epoch 15] train_acc: 0.9754196642685852, val_acc:0.9094724220623501, val_loss did not improve from 0.0275\n",
      "[Epoch 16] train_acc: 0.9623800959232613, val_acc:0.9616306954436451, val_loss improved from 0.7798 to 0.0136. Saving model to model_base_1.pth.\n",
      "[Epoch 17] train_acc: 0.9883093525179856, val_acc:0.9484412470023981, val_loss did not improve from 0.0136\n",
      "[Epoch 18] train_acc: 0.9929556354916067, val_acc:0.9826139088729017, val_loss improved from 0.6785 to 0.0003. Saving model to model_base_1.pth.\n",
      "[Epoch 19] train_acc: 0.9935551558752997, val_acc:0.9142685851318945, val_loss did not improve from 0.0003\n",
      "[Epoch 20] train_acc: 0.989658273381295, val_acc:0.9658273381294964, val_loss improved from 0.0153 to 0.0058. Saving model to model_base_1.pth.\n",
      "[Epoch 21] train_acc: 0.9934052757793765, val_acc:0.9328537170263789, val_loss did not improve from 0.0058\n",
      "[Epoch 22] train_acc: 0.9602817745803357, val_acc:0.9646282973621103, val_loss did not improve from 0.0636\n",
      "[Epoch 23] train_acc: 0.9908573141486811, val_acc:0.9814148681055156, val_loss improved from 0.0765 to 0.0016. Saving model to model_base_1.pth.\n",
      "[Epoch 24] train_acc: 0.9959532374100719, val_acc:0.9820143884892086, val_loss did not improve from 0.0016\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "from models import BaselineResNet\n",
    "model_base_1 = BaselineResNet(58)\n",
    "optimizer = optim.Adam(model_base_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_base_1, optimizer, train_loader, val_loader, epochs=25, model_path='model_base_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.4337529976019185, val_acc:0.43585131894484413, val_loss improved from inf to 1.5848. Saving model to model_base_2.pth.\n",
      "[Epoch 1] train_acc: 0.6672661870503597, val_acc:0.6768585131894485, val_loss improved from 1.5848 to 1.0063. Saving model to model_base_2.pth.\n",
      "[Epoch 2] train_acc: 0.79181654676259, val_acc:0.7140287769784173, val_loss did not improve from 1.0063\n",
      "[Epoch 3] train_acc: 0.871552757793765, val_acc:0.8381294964028777, val_loss improved from 1.4350 to 0.2368. Saving model to model_base_2.pth.\n",
      "[Epoch 4] train_acc: 0.8974820143884892, val_acc:0.8123501199040767, val_loss improved from 0.2368 to 0.0272. Saving model to model_base_2.pth.\n",
      "[Epoch 5] train_acc: 0.9262589928057554, val_acc:0.8878896882494005, val_loss did not improve from 0.0272\n",
      "[Epoch 6] train_acc: 0.954736211031175, val_acc:0.9244604316546763, val_loss improved from 0.5714 to 0.0517. Saving model to model_base_2.pth.\n",
      "[Epoch 7] train_acc: 0.9653776978417267, val_acc:0.8273381294964028, val_loss improved from 0.0517 to 0.0236. Saving model to model_base_2.pth.\n",
      "[Epoch 8] train_acc: 0.9559352517985612, val_acc:0.9274580335731415, val_loss did not improve from 0.0236\n",
      "[Epoch 9] train_acc: 0.9659772182254197, val_acc:0.9376498800959233, val_loss improved from 0.3807 to 0.2731. Saving model to model_base_2.pth.\n",
      "[Epoch 10] train_acc: 0.9844124700239808, val_acc:0.9250599520383693, val_loss did not improve from 0.2731\n",
      "[Epoch 11] train_acc: 0.9844124700239808, val_acc:0.9412470023980816, val_loss improved from 0.7542 to 0.6384. Saving model to model_base_2.pth.\n",
      "[Epoch 12] train_acc: 0.9757194244604317, val_acc:0.8657074340527577, val_loss improved from 0.6384 to 0.0147. Saving model to model_base_2.pth.\n",
      "[Epoch 13] train_acc: 0.9841127098321343, val_acc:0.9460431654676259, val_loss did not improve from 0.0147\n",
      "[Epoch 14] train_acc: 0.9832134292565947, val_acc:0.9802158273381295, val_loss improved from 0.0871 to 0.0155. Saving model to model_base_2.pth.\n",
      "[Epoch 15] train_acc: 0.9938549160671463, val_acc:0.9826139088729017, val_loss improved from 0.0155 to 0.0148. Saving model to model_base_2.pth.\n",
      "[Epoch 16] train_acc: 0.9895083932853717, val_acc:0.9622302158273381, val_loss did not improve from 0.0148\n",
      "[Epoch 17] train_acc: 0.9703237410071942, val_acc:0.9148681055155875, val_loss improved from 0.1209 to 0.0036. Saving model to model_base_2.pth.\n",
      "[Epoch 18] train_acc: 0.9857613908872902, val_acc:0.9388489208633094, val_loss did not improve from 0.0036\n",
      "[Epoch 19] train_acc: 0.9940047961630696, val_acc:0.9844124700239808, val_loss improved from 0.0721 to 0.0074. Saving model to model_base_2.pth.\n",
      "[Epoch 20] train_acc: 0.9995503597122302, val_acc:0.9886091127098321, val_loss improved from 0.0074 to 0.0007. Saving model to model_base_2.pth.\n",
      "[Epoch 21] train_acc: 1.0, val_acc:0.9922062350119905, val_loss improved from 0.0007 to 0.0002. Saving model to model_base_2.pth.\n",
      "[Epoch 22] train_acc: 1.0, val_acc:0.9934052757793765, val_loss did not improve from 0.0002\n",
      "[Epoch 23] train_acc: 1.0, val_acc:0.9922062350119905, val_loss improved from 0.0013 to 0.0002. Saving model to model_base_2.pth.\n",
      "[Epoch 24] train_acc: 1.0, val_acc:0.9922062350119905, val_loss did not improve from 0.0002\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3: Train a baseline ResNet18: no branch\n",
    "lr = 0.5 * 1e-3\n",
    "wd = 1e-4\n",
    "from models import BaselineResNet\n",
    "model_base_2 = BaselineResNet(58)\n",
    "optimizer = optim.Adam(model_base_2.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_base_2, optimizer, train_loader, val_loader, epochs=25, model_path='model_base_2.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.4009292565947242, val_acc:0.4904076738609113, val_loss improved from inf to 1.4700. Saving model to model_base_3.pth.\n",
      "[Epoch 1] train_acc: 0.5932254196642686, val_acc:0.4262589928057554, val_loss did not improve from 1.4700\n",
      "[Epoch 2] train_acc: 0.7281175059952039, val_acc:0.6534772182254197, val_loss improved from 4.2047 to 0.8472. Saving model to model_base_3.pth.\n",
      "[Epoch 3] train_acc: 0.7925659472422062, val_acc:0.7068345323741008, val_loss did not improve from 0.8472\n",
      "[Epoch 4] train_acc: 0.8694544364508393, val_acc:0.8099520383693045, val_loss improved from 1.0837 to 0.8310. Saving model to model_base_3.pth.\n",
      "[Epoch 5] train_acc: 0.8946342925659473, val_acc:0.8405275779376499, val_loss improved from 0.8310 to 0.6378. Saving model to model_base_3.pth.\n",
      "[Epoch 6] train_acc: 0.907673860911271, val_acc:0.8267386091127098, val_loss improved from 0.6378 to 0.2973. Saving model to model_base_3.pth.\n",
      "[Epoch 7] train_acc: 0.9318045563549161, val_acc:0.829136690647482, val_loss did not improve from 0.2973\n",
      "[Epoch 8] train_acc: 0.9322541966426858, val_acc:0.89568345323741, val_loss improved from 0.6692 to 0.1562. Saving model to model_base_3.pth.\n",
      "[Epoch 9] train_acc: 0.9562350119904077, val_acc:0.9214628297362111, val_loss did not improve from 0.1562\n",
      "[Epoch 10] train_acc: 0.9730215827338129, val_acc:0.9484412470023981, val_loss improved from 0.2665 to 0.0051. Saving model to model_base_3.pth.\n",
      "[Epoch 11] train_acc: 0.9782673860911271, val_acc:0.9496402877697842, val_loss did not improve from 0.0051\n",
      "[Epoch 12] train_acc: 0.9749700239808153, val_acc:0.9334532374100719, val_loss did not improve from 0.0109\n",
      "[Epoch 13] train_acc: 0.9746702637889688, val_acc:0.9430455635491607, val_loss improved from 1.2564 to 0.0942. Saving model to model_base_3.pth.\n",
      "[Epoch 14] train_acc: 0.9832134292565947, val_acc:0.9532374100719424, val_loss improved from 0.0942 to 0.0224. Saving model to model_base_3.pth.\n",
      "[Epoch 15] train_acc: 0.9869604316546763, val_acc:0.9586330935251799, val_loss improved from 0.0224 to 0.0046. Saving model to model_base_3.pth.\n",
      "[Epoch 16] train_acc: 0.9850119904076738, val_acc:0.9118705035971223, val_loss did not improve from 0.0046\n",
      "[Epoch 17] train_acc: 0.9542865707434053, val_acc:0.9526378896882494, val_loss improved from 0.7471 to 0.0651. Saving model to model_base_3.pth.\n",
      "[Epoch 18] train_acc: 0.979916067146283, val_acc:0.9718225419664268, val_loss did not improve from 0.0651\n",
      "[Epoch 19] train_acc: 0.9941546762589928, val_acc:0.947841726618705, val_loss did not improve from 0.1712\n",
      "[Epoch 20] train_acc: 0.9913069544364509, val_acc:0.9730215827338129, val_loss improved from 2.1635 to 0.5623. Saving model to model_base_3.pth.\n",
      "[Epoch 21] train_acc: 0.9865107913669064, val_acc:0.9142685851318945, val_loss improved from 0.5623 to 0.1262. Saving model to model_base_3.pth.\n",
      "[Epoch 22] train_acc: 0.9820143884892086, val_acc:0.8639088729016786, val_loss improved from 0.1262 to 0.0007. Saving model to model_base_3.pth.\n",
      "[Epoch 23] train_acc: 0.9934052757793765, val_acc:0.9790167865707434, val_loss did not improve from 0.0007\n",
      "[Epoch 24] train_acc: 0.9757194244604317, val_acc:0.7032374100719424, val_loss did not improve from 0.2185\n"
     ]
    }
   ],
   "source": [
    "# Experiment 4: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 2*1e-4\n",
    "from models import BaselineResNet\n",
    "model_base_3 = BaselineResNet(58)\n",
    "optimizer = optim.Adam(model_base_3.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_base_3, optimizer, train_loader, val_loader, epochs=25, model_path='model_base_3.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.41351918465227816, val_acc:0.32973621103117506, val_loss improved from inf to 6.4801. Saving model to model_base_4.pth.\n",
      "[Epoch 1] train_acc: 0.634742206235012, val_acc:0.5269784172661871, val_loss improved from 6.4801 to 3.0476. Saving model to model_base_4.pth.\n",
      "[Epoch 2] train_acc: 0.763189448441247, val_acc:0.5725419664268585, val_loss improved from 3.0476 to 2.2186. Saving model to model_base_4.pth.\n",
      "[Epoch 3] train_acc: 0.8339328537170264, val_acc:0.5959232613908872, val_loss improved from 2.2186 to 1.7167. Saving model to model_base_4.pth.\n",
      "[Epoch 4] train_acc: 0.8763489208633094, val_acc:0.8687050359712231, val_loss improved from 1.7167 to 0.6649. Saving model to model_base_4.pth.\n",
      "[Epoch 5] train_acc: 0.9096223021582733, val_acc:0.9058752997601919, val_loss improved from 0.6649 to 0.0973. Saving model to model_base_4.pth.\n",
      "[Epoch 6] train_acc: 0.9431954436450839, val_acc:0.9214628297362111, val_loss did not improve from 0.0973\n",
      "[Epoch 7] train_acc: 0.9577338129496403, val_acc:0.8974820143884892, val_loss improved from 0.3587 to 0.2348. Saving model to model_base_4.pth.\n",
      "[Epoch 8] train_acc: 0.966726618705036, val_acc:0.9142685851318945, val_loss did not improve from 0.2348\n",
      "[Epoch 9] train_acc: 0.9710731414868106, val_acc:0.8393285371702638, val_loss did not improve from 0.4418\n",
      "[Epoch 10] train_acc: 0.9725719424460432, val_acc:0.8866906474820144, val_loss improved from 1.4499 to 1.0610. Saving model to model_base_4.pth.\n",
      "[Epoch 11] train_acc: 0.9857613908872902, val_acc:0.882494004796163, val_loss improved from 1.0610 to 0.5224. Saving model to model_base_4.pth.\n",
      "[Epoch 12] train_acc: 0.9664268585131894, val_acc:0.934052757793765, val_loss improved from 0.5224 to 0.0138. Saving model to model_base_4.pth.\n",
      "[Epoch 13] train_acc: 0.9920563549160671, val_acc:0.9766187050359713, val_loss did not improve from 0.0138\n",
      "[Epoch 14] train_acc: 0.9968525179856115, val_acc:0.9682254196642686, val_loss improved from 0.2786 to 0.0178. Saving model to model_base_4.pth.\n",
      "[Epoch 15] train_acc: 0.9809652278177458, val_acc:0.9178657074340527, val_loss improved from 0.0178 to 0.0077. Saving model to model_base_4.pth.\n",
      "[Epoch 16] train_acc: 0.9929556354916067, val_acc:0.9850119904076738, val_loss improved from 0.0077 to 0.0009. Saving model to model_base_4.pth.\n",
      "[Epoch 17] train_acc: 0.9979016786570744, val_acc:0.9682254196642686, val_loss improved from 0.0009 to 0.0009. Saving model to model_base_4.pth.\n",
      "[Epoch 18] train_acc: 0.9983513189448441, val_acc:0.9898081534772182, val_loss did not improve from 0.0009\n",
      "[Epoch 19] train_acc: 0.9979016786570744, val_acc:0.9640287769784173, val_loss did not improve from 0.0022\n",
      "[Epoch 20] train_acc: 0.9871103117505995, val_acc:0.8633093525179856, val_loss did not improve from 0.1029\n",
      "[Epoch 21] train_acc: 0.986810551558753, val_acc:0.9508393285371702, val_loss improved from 1.0041 to 0.0221. Saving model to model_base_4.pth.\n",
      "[Epoch 22] train_acc: 0.9947541966426858, val_acc:0.9364508393285371, val_loss did not improve from 0.0221\n",
      "[Epoch 23] train_acc: 0.997751798561151, val_acc:0.9826139088729017, val_loss improved from 0.0247 to 0.0014. Saving model to model_base_4.pth.\n",
      "[Epoch 24] train_acc: 0.9976019184652278, val_acc:0.9898081534772182, val_loss improved from 0.0014 to 0.0009. Saving model to model_base_4.pth.\n",
      "[Epoch 25] train_acc: 0.9995503597122302, val_acc:0.9640287769784173, val_loss did not improve from 0.0009\n",
      "[Epoch 26] train_acc: 0.9746702637889688, val_acc:0.7422062350119905, val_loss improved from 1.4605 to 0.1028. Saving model to model_base_4.pth.\n",
      "[Epoch 27] train_acc: 0.9845623501199041, val_acc:0.9394484412470024, val_loss did not improve from 0.1028\n",
      "[Epoch 28] train_acc: 0.9937050359712231, val_acc:0.9790167865707434, val_loss improved from 0.1425 to 0.0017. Saving model to model_base_4.pth.\n",
      "[Epoch 29] train_acc: 0.9964028776978417, val_acc:0.9904076738609112, val_loss did not improve from 0.0017\n"
     ]
    }
   ],
   "source": [
    "# Experiment 5: Train a baseline ResNet18: no branch\n",
    "lr = 0.8*1e-3\n",
    "wd = 1e-4\n",
    "from models import BaselineResNet\n",
    "model_base_4 = BaselineResNet(58)\n",
    "optimizer = optim.Adam(model_base_4.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_base_4, optimizer, train_loader, val_loader, epochs=30, model_path='model_base_4.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.052158273381294966, val_acc:0.1552757793764988, val_loss improved from inf to 3.9343. Saving model to model_base_5.pth.\n",
      "[Epoch 1] train_acc: 0.1474820143884892, val_acc:0.15347721822541965, val_loss improved from 3.9343 to 3.3593. Saving model to model_base_5.pth.\n",
      "[Epoch 2] train_acc: 0.15197841726618705, val_acc:0.16606714628297362, val_loss did not improve from 3.3593\n",
      "[Epoch 3] train_acc: 0.18854916067146282, val_acc:0.2014388489208633, val_loss did not improve from 3.5582\n",
      "[Epoch 4] train_acc: 0.22422062350119903, val_acc:0.2392086330935252, val_loss improved from 3.6231 to 2.2558. Saving model to model_base_5.pth.\n",
      "[Epoch 5] train_acc: 0.25524580335731417, val_acc:0.2607913669064748, val_loss improved from 2.2558 to 2.0427. Saving model to model_base_5.pth.\n",
      "[Epoch 6] train_acc: 0.27473021582733814, val_acc:0.2637889688249401, val_loss did not improve from 2.0427\n",
      "[Epoch 7] train_acc: 0.28042565947242204, val_acc:0.2709832134292566, val_loss improved from 3.5269 to 2.6137. Saving model to model_base_5.pth.\n",
      "[Epoch 8] train_acc: 0.2873201438848921, val_acc:0.28177458033573144, val_loss did not improve from 2.6137\n",
      "[Epoch 9] train_acc: 0.29361510791366907, val_acc:0.27997601918465226, val_loss improved from 3.3509 to 2.1292. Saving model to model_base_5.pth.\n",
      "[Epoch 10] train_acc: 0.2966127098321343, val_acc:0.2853717026378897, val_loss did not improve from 2.1292\n",
      "[Epoch 11] train_acc: 0.3036570743405276, val_acc:0.3009592326139089, val_loss did not improve from 2.5096\n",
      "[Epoch 12] train_acc: 0.31459832134292565, val_acc:0.3135491606714628, val_loss improved from 3.7447 to 2.8653. Saving model to model_base_5.pth.\n",
      "[Epoch 13] train_acc: 0.32538968824940045, val_acc:0.30635491606714627, val_loss did not improve from 2.8653\n",
      "[Epoch 14] train_acc: 0.3342326139088729, val_acc:0.32793764988009594, val_loss improved from 3.5208 to 2.3687. Saving model to model_base_5.pth.\n",
      "[Epoch 15] train_acc: 0.342326139088729, val_acc:0.33992805755395683, val_loss did not improve from 2.3687\n",
      "[Epoch 16] train_acc: 0.35311750599520386, val_acc:0.3351318944844125, val_loss improved from 3.6502 to 1.6561. Saving model to model_base_5.pth.\n",
      "[Epoch 17] train_acc: 0.35656474820143885, val_acc:0.3459232613908873, val_loss did not improve from 1.6561\n",
      "[Epoch 18] train_acc: 0.3669064748201439, val_acc:0.34772182254196643, val_loss improved from 2.9909 to 1.6898. Saving model to model_base_5.pth.\n",
      "[Epoch 19] train_acc: 0.3675059952038369, val_acc:0.3609112709832134, val_loss did not improve from 1.6898\n",
      "[Epoch 20] train_acc: 0.3712529976019185, val_acc:0.3723021582733813, val_loss did not improve from 2.3049\n",
      "[Epoch 21] train_acc: 0.37934652278177455, val_acc:0.36510791366906475, val_loss improved from 2.5469 to 2.0300. Saving model to model_base_5.pth.\n",
      "[Epoch 22] train_acc: 0.38594124700239807, val_acc:0.37889688249400477, val_loss improved from 2.0300 to 1.3892. Saving model to model_base_5.pth.\n",
      "[Epoch 23] train_acc: 0.3940347721822542, val_acc:0.38189448441247004, val_loss did not improve from 1.3892\n",
      "[Epoch 24] train_acc: 0.3986810551558753, val_acc:0.38669064748201437, val_loss improved from 2.8673 to 2.4786. Saving model to model_base_5.pth.\n",
      "[Epoch 25] train_acc: 0.4028776978417266, val_acc:0.37889688249400477, val_loss improved from 2.4786 to 1.0568. Saving model to model_base_5.pth.\n",
      "[Epoch 26] train_acc: 0.4088729016786571, val_acc:0.38369304556354916, val_loss did not improve from 1.0568\n",
      "[Epoch 27] train_acc: 0.4154676258992806, val_acc:0.40167865707434053, val_loss did not improve from 1.8888\n",
      "[Epoch 28] train_acc: 0.42281175059952036, val_acc:0.41127098321342925, val_loss did not improve from 2.1755\n",
      "[Epoch 29] train_acc: 0.42880695443645084, val_acc:0.4238609112709832, val_loss improved from 3.1475 to 1.4832. Saving model to model_base_5.pth.\n",
      "[Epoch 30] train_acc: 0.4301558752997602, val_acc:0.4304556354916067, val_loss did not improve from 1.4832\n",
      "[Epoch 31] train_acc: 0.4392985611510791, val_acc:0.43465227817745805, val_loss improved from 2.7337 to 0.9365. Saving model to model_base_5.pth.\n",
      "[Epoch 32] train_acc: 0.4455935251798561, val_acc:0.37110311750599523, val_loss did not improve from 0.9365\n",
      "[Epoch 33] train_acc: 0.44859112709832133, val_acc:0.4370503597122302, val_loss improved from 2.5841 to 1.6784. Saving model to model_base_5.pth.\n",
      "[Epoch 34] train_acc: 0.45863309352517984, val_acc:0.45323741007194246, val_loss improved from 1.6784 to 1.2661. Saving model to model_base_5.pth.\n",
      "[Epoch 35] train_acc: 0.46327937649880097, val_acc:0.4430455635491607, val_loss did not improve from 1.2661\n",
      "[Epoch 36] train_acc: 0.46447841726618705, val_acc:0.46522781774580335, val_loss did not improve from 2.0523\n",
      "[Epoch 37] train_acc: 0.4746702637889688, val_acc:0.46822541966426856, val_loss did not improve from 2.7300\n",
      "[Epoch 38] train_acc: 0.4802158273381295, val_acc:0.395083932853717, val_loss improved from 3.4461 to 1.3309. Saving model to model_base_5.pth.\n",
      "[Epoch 39] train_acc: 0.48531175059952036, val_acc:0.45623501199040767, val_loss did not improve from 1.3309\n",
      "[Epoch 40] train_acc: 0.48815947242206237, val_acc:0.4052757793764988, val_loss did not improve from 1.8701\n",
      "[Epoch 41] train_acc: 0.4982014388489209, val_acc:0.48860911270983215, val_loss improved from 2.4376 to 1.6670. Saving model to model_base_5.pth.\n",
      "[Epoch 42] train_acc: 0.5040467625899281, val_acc:0.4970023980815348, val_loss improved from 1.6670 to 0.0537. Saving model to model_base_5.pth.\n",
      "[Epoch 43] train_acc: 0.5088429256594724, val_acc:0.5011990407673861, val_loss did not improve from 0.0537\n",
      "[Epoch 44] train_acc: 0.5133393285371702, val_acc:0.4862110311750599, val_loss improved from 2.1124 to 0.9451. Saving model to model_base_5.pth.\n",
      "[Epoch 45] train_acc: 0.5188848920863309, val_acc:0.513189448441247, val_loss did not improve from 0.9451\n",
      "[Epoch 46] train_acc: 0.5295263788968825, val_acc:0.5101918465227818, val_loss did not improve from 1.8858\n",
      "[Epoch 47] train_acc: 0.5259292565947242, val_acc:0.4706235011990408, val_loss did not improve from 2.2643\n",
      "[Epoch 48] train_acc: 0.529826139088729, val_acc:0.5257793764988009, val_loss improved from 2.4857 to 1.4433. Saving model to model_base_5.pth.\n",
      "[Epoch 49] train_acc: 0.5359712230215827, val_acc:0.5251798561151079, val_loss did not improve from 1.4433\n",
      "[Epoch 50] train_acc: 0.5391187050359713, val_acc:0.5221822541966427, val_loss improved from 1.8203 to 1.0307. Saving model to model_base_5.pth.\n",
      "[Epoch 51] train_acc: 0.5443645083932853, val_acc:0.5305755395683454, val_loss did not improve from 1.0307\n",
      "[Epoch 52] train_acc: 0.5478117505995204, val_acc:0.5287769784172662, val_loss did not improve from 1.2057\n",
      "[Epoch 53] train_acc: 0.5551558752997602, val_acc:0.5335731414868106, val_loss did not improve from 1.7277\n",
      "[Epoch 54] train_acc: 0.5592026378896883, val_acc:0.5329736211031175, val_loss improved from 2.1519 to 2.0792. Saving model to model_base_5.pth.\n",
      "[Epoch 55] train_acc: 0.5641486810551559, val_acc:0.5377697841726619, val_loss improved from 2.0792 to 1.5375. Saving model to model_base_5.pth.\n",
      "[Epoch 56] train_acc: 0.563099520383693, val_acc:0.5047961630695443, val_loss did not improve from 1.5375\n",
      "[Epoch 57] train_acc: 0.5680455635491607, val_acc:0.4706235011990408, val_loss improved from 2.6967 to 1.5076. Saving model to model_base_5.pth.\n",
      "[Epoch 58] train_acc: 0.5677458033573142, val_acc:0.5569544364508393, val_loss improved from 1.5076 to 1.0575. Saving model to model_base_5.pth.\n",
      "[Epoch 59] train_acc: 0.5801858513189448, val_acc:0.5293764988009593, val_loss did not improve from 1.0575\n",
      "[Epoch 60] train_acc: 0.5806354916067147, val_acc:0.526378896882494, val_loss improved from 2.9328 to 2.2396. Saving model to model_base_5.pth.\n",
      "[Epoch 61] train_acc: 0.5842326139088729, val_acc:0.5461630695443646, val_loss improved from 2.2396 to 1.5714. Saving model to model_base_5.pth.\n",
      "[Epoch 62] train_acc: 0.5897781774580336, val_acc:0.564748201438849, val_loss did not improve from 1.5714\n",
      "[Epoch 63] train_acc: 0.5932254196642686, val_acc:0.5359712230215827, val_loss improved from 1.5806 to 0.5891. Saving model to model_base_5.pth.\n",
      "[Epoch 64] train_acc: 0.5971223021582733, val_acc:0.5719424460431655, val_loss did not improve from 0.5891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 65] train_acc: 0.5984712230215827, val_acc:0.5833333333333334, val_loss improved from 2.5253 to 1.4069. Saving model to model_base_5.pth.\n",
      "[Epoch 66] train_acc: 0.6032673860911271, val_acc:0.5377697841726619, val_loss did not improve from 1.4069\n",
      "[Epoch 67] train_acc: 0.6059652278177458, val_acc:0.5755395683453237, val_loss improved from 2.2139 to 1.9056. Saving model to model_base_5.pth.\n",
      "[Epoch 68] train_acc: 0.6095623501199041, val_acc:0.579136690647482, val_loss improved from 1.9056 to 1.4252. Saving model to model_base_5.pth.\n",
      "[Epoch 69] train_acc: 0.6101618705035972, val_acc:0.5785371702637889, val_loss did not improve from 1.4252\n"
     ]
    }
   ],
   "source": [
    "# Experiment 6: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "from models import BaselineResNet\n",
    "model_base_5 = BaselineResNet(58)\n",
    "optimizer = optim.SGD(model_base_5.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_base_5, optimizer, train_loader, val_loader, epochs=70, model_path='model_base_5.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.612410071942446, val_acc:0.5785371702637889, val_loss improved from inf to 2.0093. Saving model to model_base_6.pth.\n",
      "[Epoch 1] train_acc: 0.612410071942446, val_acc:0.5053956834532374, val_loss did not improve from 2.0093\n",
      "[Epoch 2] train_acc: 0.6157074340527577, val_acc:0.5893285371702638, val_loss improved from 2.5192 to 1.3822. Saving model to model_base_6.pth.\n",
      "[Epoch 3] train_acc: 0.6151079136690647, val_acc:0.5623501199040767, val_loss improved from 1.3822 to 1.2687. Saving model to model_base_6.pth.\n",
      "[Epoch 4] train_acc: 0.6172062350119905, val_acc:0.5599520383693045, val_loss did not improve from 1.2687\n",
      "[Epoch 5] train_acc: 0.6193045563549161, val_acc:0.5833333333333334, val_loss improved from 2.4156 to 2.0095. Saving model to model_base_6.pth.\n",
      "[Epoch 6] train_acc: 0.6267985611510791, val_acc:0.4982014388489209, val_loss improved from 2.0095 to 1.4769. Saving model to model_base_6.pth.\n",
      "[Epoch 7] train_acc: 0.6263489208633094, val_acc:0.5185851318944844, val_loss did not improve from 1.4769\n",
      "[Epoch 8] train_acc: 0.6305455635491607, val_acc:0.5629496402877698, val_loss improved from 1.9561 to 1.3243. Saving model to model_base_6.pth.\n",
      "[Epoch 9] train_acc: 0.6306954436450839, val_acc:0.579736211031175, val_loss improved from 1.3243 to 1.1047. Saving model to model_base_6.pth.\n",
      "[Epoch 10] train_acc: 0.6345923261390888, val_acc:0.6079136690647482, val_loss improved from 1.1047 to 0.4386. Saving model to model_base_6.pth.\n",
      "[Epoch 11] train_acc: 0.641636690647482, val_acc:0.5329736211031175, val_loss did not improve from 0.4386\n",
      "[Epoch 12] train_acc: 0.6377398081534772, val_acc:0.5905275779376499, val_loss improved from 1.9166 to 0.6383. Saving model to model_base_6.pth.\n",
      "[Epoch 13] train_acc: 0.6401378896882494, val_acc:0.604916067146283, val_loss did not improve from 0.6383\n",
      "[Epoch 14] train_acc: 0.6458333333333334, val_acc:0.5755395683453237, val_loss improved from 2.2474 to 0.7221. Saving model to model_base_6.pth.\n",
      "[Epoch 15] train_acc: 0.6458333333333334, val_acc:0.6169064748201439, val_loss did not improve from 0.7221\n",
      "[Epoch 16] train_acc: 0.6540767386091128, val_acc:0.5887290167865707, val_loss did not improve from 3.2125\n",
      "[Epoch 17] train_acc: 0.6498800959232613, val_acc:0.6097122302158273, val_loss improved from 3.3036 to 1.4555. Saving model to model_base_6.pth.\n",
      "[Epoch 18] train_acc: 0.6540767386091128, val_acc:0.5731414868105515, val_loss did not improve from 1.4555\n",
      "[Epoch 19] train_acc: 0.657673860911271, val_acc:0.60431654676259, val_loss did not improve from 1.5918\n",
      "[Epoch 20] train_acc: 0.6572242206235012, val_acc:0.6139088729016786, val_loss improved from 2.5452 to 1.8423. Saving model to model_base_6.pth.\n",
      "[Epoch 21] train_acc: 0.6627697841726619, val_acc:0.5299760191846523, val_loss improved from 1.8423 to 1.0128. Saving model to model_base_6.pth.\n",
      "[Epoch 22] train_acc: 0.6605215827338129, val_acc:0.6312949640287769, val_loss did not improve from 1.0128\n",
      "[Epoch 23] train_acc: 0.6660671462829736, val_acc:0.5617505995203836, val_loss improved from 1.9796 to 1.5219. Saving model to model_base_6.pth.\n",
      "[Epoch 24] train_acc: 0.6735611510791367, val_acc:0.540167865707434, val_loss did not improve from 1.5219\n",
      "[Epoch 25] train_acc: 0.6725119904076738, val_acc:0.6211031175059952, val_loss improved from 1.5585 to 0.8424. Saving model to model_base_6.pth.\n",
      "[Epoch 26] train_acc: 0.66681654676259, val_acc:0.6402877697841727, val_loss improved from 0.8424 to 0.8087. Saving model to model_base_6.pth.\n",
      "[Epoch 27] train_acc: 0.674310551558753, val_acc:0.6510791366906474, val_loss improved from 0.8087 to 0.8000. Saving model to model_base_6.pth.\n",
      "[Epoch 28] train_acc: 0.6801558752997602, val_acc:0.6432853717026379, val_loss did not improve from 0.8000\n",
      "[Epoch 29] train_acc: 0.6849520383693045, val_acc:0.6426858513189448, val_loss improved from 2.6038 to 1.2202. Saving model to model_base_6.pth.\n",
      "[Epoch 30] train_acc: 0.6873501199040767, val_acc:0.6288968824940048, val_loss improved from 1.2202 to 1.0838. Saving model to model_base_6.pth.\n",
      "[Epoch 31] train_acc: 0.6879496402877698, val_acc:0.500599520383693, val_loss did not improve from 1.0838\n",
      "[Epoch 32] train_acc: 0.6857014388489209, val_acc:0.6612709832134293, val_loss improved from 1.1733 to 0.7228. Saving model to model_base_6.pth.\n",
      "[Epoch 33] train_acc: 0.6894484412470024, val_acc:0.6654676258992805, val_loss did not improve from 0.7228\n",
      "[Epoch 34] train_acc: 0.6892985611510791, val_acc:0.6546762589928058, val_loss improved from 1.1817 to 0.9521. Saving model to model_base_6.pth.\n",
      "[Epoch 35] train_acc: 0.6985911270983214, val_acc:0.6624700239808153, val_loss did not improve from 0.9521\n",
      "[Epoch 36] train_acc: 0.7030875299760192, val_acc:0.4592326139088729, val_loss did not improve from 1.2338\n",
      "[Epoch 37] train_acc: 0.6981414868105515, val_acc:0.6354916067146283, val_loss improved from 1.6491 to 0.5486. Saving model to model_base_6.pth.\n",
      "[Epoch 38] train_acc: 0.7072841726618705, val_acc:0.6199040767386091, val_loss improved from 0.5486 to 0.1748. Saving model to model_base_6.pth.\n",
      "[Epoch 39] train_acc: 0.7122302158273381, val_acc:0.6169064748201439, val_loss did not improve from 0.1748\n",
      "[Epoch 40] train_acc: 0.7084832134292566, val_acc:0.6079136690647482, val_loss improved from 2.8564 to 0.7701. Saving model to model_base_6.pth.\n",
      "[Epoch 41] train_acc: 0.7105815347721822, val_acc:0.644484412470024, val_loss did not improve from 0.7701\n",
      "[Epoch 42] train_acc: 0.7161270983213429, val_acc:0.6342925659472423, val_loss improved from 1.0471 to 0.5849. Saving model to model_base_6.pth.\n",
      "[Epoch 43] train_acc: 0.7168764988009593, val_acc:0.5503597122302158, val_loss did not improve from 0.5849\n",
      "[Epoch 44] train_acc: 0.7198741007194245, val_acc:0.6618705035971223, val_loss improved from 2.1054 to 0.2515. Saving model to model_base_6.pth.\n",
      "[Epoch 45] train_acc: 0.7216726618705036, val_acc:0.6235011990407674, val_loss did not improve from 0.2515\n",
      "[Epoch 46] train_acc: 0.7228717026378897, val_acc:0.6438848920863309, val_loss did not improve from 1.1030\n",
      "[Epoch 47] train_acc: 0.7288669064748201, val_acc:0.6552757793764988, val_loss improved from 2.0155 to 1.7726. Saving model to model_base_6.pth.\n",
      "[Epoch 48] train_acc: 0.7290167865707434, val_acc:0.5755395683453237, val_loss did not improve from 1.7726\n",
      "[Epoch 49] train_acc: 0.7296163069544365, val_acc:0.5773381294964028, val_loss improved from 2.2893 to 1.4951. Saving model to model_base_6.pth.\n",
      "[Epoch 50] train_acc: 0.7291666666666666, val_acc:0.43285371702637887, val_loss did not improve from 1.4951\n",
      "[Epoch 51] train_acc: 0.7290167865707434, val_acc:0.6750599520383693, val_loss improved from 2.5802 to 2.5322. Saving model to model_base_6.pth.\n",
      "[Epoch 52] train_acc: 0.7422062350119905, val_acc:0.6744604316546763, val_loss improved from 2.5322 to 1.0649. Saving model to model_base_6.pth.\n",
      "[Epoch 53] train_acc: 0.7372601918465228, val_acc:0.5329736211031175, val_loss did not improve from 1.0649\n",
      "[Epoch 54] train_acc: 0.7372601918465228, val_acc:0.6540767386091128, val_loss did not improve from 1.7121\n",
      "[Epoch 55] train_acc: 0.74310551558753, val_acc:0.605515587529976, val_loss improved from 2.0747 to 1.7397. Saving model to model_base_6.pth.\n",
      "[Epoch 56] train_acc: 0.7435551558752997, val_acc:0.6996402877697842, val_loss improved from 1.7397 to 0.3131. Saving model to model_base_6.pth.\n",
      "[Epoch 57] train_acc: 0.7479016786570744, val_acc:0.7086330935251799, val_loss did not improve from 0.3131\n",
      "[Epoch 58] train_acc: 0.7546462829736211, val_acc:0.6546762589928058, val_loss did not improve from 1.0936\n",
      "[Epoch 59] train_acc: 0.7553956834532374, val_acc:0.6822541966426858, val_loss improved from 1.6356 to 0.8085. Saving model to model_base_6.pth.\n",
      "[Epoch 60] train_acc: 0.7550959232613909, val_acc:0.604916067146283, val_loss did not improve from 0.8085\n",
      "[Epoch 61] train_acc: 0.7547961630695443, val_acc:0.6109112709832134, val_loss improved from 2.1175 to 1.2961. Saving model to model_base_6.pth.\n",
      "[Epoch 62] train_acc: 0.7561450839328537, val_acc:0.5971223021582733, val_loss improved from 1.2961 to 0.4018. Saving model to model_base_6.pth.\n",
      "[Epoch 63] train_acc: 0.7579436450839329, val_acc:0.5707434052757794, val_loss did not improve from 0.4018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 64] train_acc: 0.7636390887290168, val_acc:0.697242206235012, val_loss did not improve from 0.6093\n",
      "[Epoch 65] train_acc: 0.7658872901678657, val_acc:0.6816546762589928, val_loss improved from 1.8317 to 1.4549. Saving model to model_base_6.pth.\n",
      "[Epoch 66] train_acc: 0.7648381294964028, val_acc:0.7194244604316546, val_loss improved from 1.4549 to 0.7013. Saving model to model_base_6.pth.\n",
      "[Epoch 67] train_acc: 0.770083932853717, val_acc:0.4922062350119904, val_loss did not improve from 0.7013\n",
      "[Epoch 68] train_acc: 0.7661870503597122, val_acc:0.6900479616306955, val_loss did not improve from 0.7527\n",
      "[Epoch 69] train_acc: 0.7693345323741008, val_acc:0.7170263788968825, val_loss did not improve from 1.0467\n"
     ]
    }
   ],
   "source": [
    "# Experiment 7: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "from models import BaselineResNet\n",
    "model_base_6 = BaselineResNet(58)\n",
    "model_path = 'model_base_5.pth'\n",
    "params = torch.load(model_path)\n",
    "model_base_6.load_state_dict(params)\n",
    "model_base_6 = model_base_6.to(device=device)\n",
    "optimizer = optim.SGD(model_base_6.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_base_6, optimizer, train_loader, val_loader, epochs=70, model_path='model_base_6.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.7721822541966427, val_acc:0.6996402877697842, val_loss improved from inf to 0.7670. Saving model to model_base_7.pth.\n",
      "[Epoch 1] train_acc: 0.7733812949640287, val_acc:0.6936450839328537, val_loss did not improve from 0.7670\n",
      "[Epoch 2] train_acc: 0.7705335731414868, val_acc:0.5353717026378897, val_loss improved from 1.0371 to 0.8967. Saving model to model_base_7.pth.\n",
      "[Epoch 3] train_acc: 0.7726318944844125, val_acc:0.7134292565947242, val_loss did not improve from 0.8967\n",
      "[Epoch 4] train_acc: 0.7741306954436451, val_acc:0.5695443645083933, val_loss improved from 2.0632 to 1.1265. Saving model to model_base_7.pth.\n",
      "[Epoch 5] train_acc: 0.7774280575539568, val_acc:0.6870503597122302, val_loss did not improve from 1.1265\n",
      "[Epoch 6] train_acc: 0.7784772182254197, val_acc:0.7008393285371702, val_loss improved from 1.2648 to 0.4302. Saving model to model_base_7.pth.\n",
      "[Epoch 7] train_acc: 0.7831235011990407, val_acc:0.7482014388489209, val_loss did not improve from 0.4302\n",
      "[Epoch 8] train_acc: 0.7847721822541966, val_acc:0.6660671462829736, val_loss did not improve from 0.6327\n",
      "[Epoch 9] train_acc: 0.7852218225419664, val_acc:0.6672661870503597, val_loss improved from 1.7651 to 0.2904. Saving model to model_base_7.pth.\n",
      "[Epoch 10] train_acc: 0.7823741007194245, val_acc:0.7146282973621103, val_loss did not improve from 0.2904\n",
      "[Epoch 11] train_acc: 0.7838729016786571, val_acc:0.5491606714628298, val_loss did not improve from 1.2484\n",
      "[Epoch 12] train_acc: 0.7909172661870504, val_acc:0.6678657074340527, val_loss improved from 3.0781 to 0.6684. Saving model to model_base_7.pth.\n",
      "[Epoch 13] train_acc: 0.789568345323741, val_acc:0.6354916067146283, val_loss did not improve from 0.6684\n",
      "[Epoch 14] train_acc: 0.7898681055155875, val_acc:0.6822541966426858, val_loss improved from 1.4181 to 0.3867. Saving model to model_base_7.pth.\n",
      "[Epoch 15] train_acc: 0.7948141486810552, val_acc:0.7422062350119905, val_loss improved from 0.3867 to 0.0980. Saving model to model_base_7.pth.\n",
      "[Epoch 16] train_acc: 0.7991606714628298, val_acc:0.6462829736211031, val_loss did not improve from 0.0980\n",
      "[Epoch 17] train_acc: 0.7981115107913669, val_acc:0.736810551558753, val_loss improved from 1.9409 to 0.5394. Saving model to model_base_7.pth.\n",
      "[Epoch 18, Batch 49] train_loss: 0.7352318763732915"
     ]
    }
   ],
   "source": [
    "# Experiment 8: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "from models import BaselineResNet\n",
    "model_base_7 = BaselineResNet(58)\n",
    "model_path = 'model_base_6.pth'\n",
    "params = torch.load(model_path)\n",
    "model_base_7.load_state_dict(params)\n",
    "model_base_7 = model_base_7.to(device=device)\n",
    "optimizer = optim.SGD(model_base_7.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_base_7, optimizer, train_loader, val_loader, epochs=70, model_path='model_base_7.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Uncorrupted Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.2637951374053955, 0.6228686058174524)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import BaselineResNet\n",
    "model_path = 'model.pth'\n",
    "model = BaselineResNet(58)\n",
    "params = torch.load(model_path)\n",
    "model.load_state_dict(params)\n",
    "model = model.to(device=device)\n",
    "evaluate_main(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1314829587936401, 0.6855566700100301)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import BaselineResNet\n",
    "model_path = 'model_base_1.pth'\n",
    "model_base_1 = BaselineResNet(58)\n",
    "params = torch.load(model_path)\n",
    "model_base_1.load_state_dict(params)\n",
    "model_base_1 = model_base_1.to(device=device)\n",
    "evaluate_main(model_base_1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.4556317329406738, 0.6639919759277834)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import BaselineResNet\n",
    "model_path = 'model_base_2.pth'\n",
    "model_base_2 = BaselineResNet(58)\n",
    "params = torch.load(model_path)\n",
    "model_base_2.load_state_dict(params)\n",
    "model_base_2 = model_base_2.to(device=device)\n",
    "evaluate_main(model_base_2, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.6625049114227295, 0.5125376128385155)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import BaselineResNet\n",
    "model_path = 'model_base_3.pth'\n",
    "model_base_3 = BaselineResNet(58)\n",
    "params = torch.load(model_path)\n",
    "model_base_3.load_state_dict(params)\n",
    "model_base_3 = model_base_3.to(device=device)\n",
    "evaluate_main(model_base_3, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\hycme/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.644360065460205, 0.645937813440321)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import BaselineResNet\n",
    "model_path = 'model_base_4.pth'\n",
    "model_base_4 = BaselineResNet(58)\n",
    "params = torch.load(model_path)\n",
    "model_base_4.load_state_dict(params)\n",
    "model_base_4 = model_base_4.to(device=device)\n",
    "evaluate_main(model_base_4, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: ResNet18 with Auxillary Branch (No Online Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Train a ResNet18 with auxillary branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: ResNet18 with Auxillary Branch (Online-Trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Do online training on the auxillary branch, with pre-trained shared and main branch weights from experiment 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
