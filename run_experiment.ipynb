{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "## Description\n",
    "\n",
    "## Pre-processing\n",
    "\n",
    "### Resizing\n",
    "\n",
    "### Random cropping\n",
    "\n",
    "### Corrupting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "batch_size = 1024\n",
    "train_mean = [86.69585, 86.342995, 85.84817]\n",
    "train_std = [74.59906, 74.196365, 73.890495]\n",
    "# Preprocessing\n",
    "transform = T.Compose([\n",
    "                T.Normalize(train_mean, train_std)\n",
    "            ])\n",
    "\n",
    "#train_set = TensorDataset(torch.load('train_x.pt'), torch.load('train_y.pt'))\n",
    "#val_set = TensorDataset(torch.load('val_x.pt'), torch.load('val_y.pt'))\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "\n",
    "train_rotate_set = TensorDataset(torch.load('train_x_rotate.pt'), torch.load('train_y_rotate.pt'))\n",
    "val_rotate_set = TensorDataset(torch.load('val_x_rotate.pt'), torch.load('val_y_rotate.pt'))\n",
    "test_rotate_set = TensorDataset(torch.load('test_x_rotate.pt'), torch.load('test_y_rotate.pt'))\n",
    "\n",
    "#train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_rotate_loader = DataLoader(train_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "val_rotate_loader = DataLoader(val_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "test_rotate_loader = DataLoader(test_rotate_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Procedures\n",
    "\n",
    "## Regular Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training pipelines\n",
    "def train_main(model, optimizer, loader_train, loader_val, epochs=1, model_path=None, early_stop_patience = 0):\n",
    "    \"\"\"\n",
    "    Train the main branch\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Logger object with loss and accuracy data\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    logger = Logger()\n",
    "    last_loss = float('inf')\n",
    "    for e in range(epochs):\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        total_loss = 0.0\n",
    "        count = 0\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"\\r[Epoch {e + 1}, Batch {t}] train_loss: {loss.item()}\", end='')\n",
    "            count += 1\n",
    "\n",
    "        # Conclude Epoch\n",
    "        train_loss = total_loss / count\n",
    "        train_acc = float(num_correct) / num_samples\n",
    "        val_loss, val_acc = evaluate_main(model, loader_val)\n",
    "        logger.log(train_loss, train_acc, val_loss, val_acc)\n",
    "        \n",
    "        with open(model_path.split('.')[0] + '.pkl', 'wb') as output_file:\n",
    "            pickle.dump(logger, output_file)\n",
    "\n",
    "        # Early Stopping\n",
    "        if logger.check_early_stop(early_stop_patience):\n",
    "            print(\"[Early Stopped]\")\n",
    "            break\n",
    "        else:\n",
    "            if last_loss > val_loss:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss improved from %.4f to %.4f. Saving model to {model_path}.\" % (last_loss, val_loss))\n",
    "                if model_path is not None:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss did not improve from %.4f\" % (last_loss))\n",
    "            last_loss = val_loss\n",
    "    return logger\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_both(model, optimizer, loader_train, loader_val, epochs=1, model_path=None, early_stop_patience = 0):\n",
    "    \"\"\"\n",
    "    Train the main and auxillary branch\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Logger object with loss and accuracy data\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    logger = Logger()\n",
    "    last_loss = float('inf')\n",
    "    for e in range(epochs):\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss_main = F.cross_entropy(scores[0], y[:, 0])\n",
    "            loss_auxillary = F.cross_entropy(scores[1], y[:, 1])\n",
    "            loss = loss_main + loss_auxillary\n",
    "            running_loss += loss_main.item()\n",
    "\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (preds == y[:, 0]).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\", end='')\n",
    "            count += 1\n",
    "\n",
    "        # Conclude Epoch\n",
    "        train_loss = running_loss / count\n",
    "        train_acc = float(num_correct) / num_samples\n",
    "        val_loss, val_acc = evaluate_both(model, loader_val)\n",
    "        logger.log(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "        with open(model_path.split('.')[0] + '.pkl', 'wb') as output_file:\n",
    "            pickle.dump(logger, output_file)\n",
    "            \n",
    "        # Early Stopping\n",
    "        if logger.check_early_stop(early_stop_patience):\n",
    "            print(\"[Early Stopped]\")\n",
    "            break\n",
    "        else:\n",
    "            if last_loss > val_loss:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss improved from %.4f to %.4f. Saving model to {model_path}.\" % (last_loss, val_loss))\n",
    "                if model_path is not None:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss did not improve from %.4f\" % (last_loss))\n",
    "            last_loss = val_loss\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_main(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    ave_loss = 0.0\n",
    "    count = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            ave_loss += loss.item()\n",
    "            count += 1\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return ave_loss / count, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_both(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy in model with two predictions\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    ave_loss = 0.0\n",
    "    count = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss_main = F.cross_entropy(scores[0], y[:, 0])\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (preds == y[:, 0]).sum()\n",
    "            # print(f\"num_correct: {num_correct}\")\n",
    "            num_samples += preds.size(0)\n",
    "            # print(f\"num_samples: {num_samples}\")\n",
    "            ave_loss += loss_main.item()\n",
    "            count += 1\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return ave_loss / count, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_non_rotate(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy in model with two predictions\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    from random import randrange\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    ave_loss = 0.0\n",
    "    count = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "#             random_number = randrange(4)\n",
    "#             if random_number == 0:\n",
    "#                 pass\n",
    "#             elif random_number == 1:\n",
    "#                 for i in range(x.size()[0]):\n",
    "#                     x[i][0] = torch.rot90(x[i][0])\n",
    "#                     x[i][1] = torch.rot90(x[i][1])\n",
    "#                     x[i][2] = torch.rot90(x[i][2])\n",
    "#             elif random_number == 2:\n",
    "#                 for i in range(x.size()[0]):\n",
    "#                     x[i][0] = torch.rot90(x[i][0], 2)\n",
    "#                     x[i][1] = torch.rot90(x[i][1], 2)\n",
    "#                     x[i][2] = torch.rot90(x[i][2], 2)\n",
    "#             elif random_number == 3:\n",
    "#                 for i in range(x.size()[0]):\n",
    "#                     x[i][0] = torch.rot90(x[i][0], 3)\n",
    "#                     x[i][1] = torch.rot90(x[i][1], 3)\n",
    "#                     x[i][2] = torch.rot90(x[i][2], 3)\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss_main = F.cross_entropy(scores[0], y)\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            #print(f\"num_correct: {num_correct}\")\n",
    "            num_samples += preds.size(0)\n",
    "            #print(f\"num_samples: {num_samples}\")\n",
    "            ave_loss += loss_main.item()\n",
    "            count += 1\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return ave_loss / count, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Time Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttt(model, loader, loader_spinned, optimizer):\n",
    "    \"\"\"\n",
    "    TTT with image spinning task\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for x, y in loader_spinned:\n",
    "        x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        scores = model(x)\n",
    "        loss_auxillary = F.cross_entropy(scores[1], y[:, 1])\n",
    "        optimizer.zero_grad()\n",
    "        loss_auxillary.backward()\n",
    "        optimizer.step()\n",
    "    return evaluate_non_rotate(model, loader)\n",
    "\n",
    "def ttt_online(model, loader, loader_spinned, optimizer):\n",
    "    \"\"\"\n",
    "    Online TTT with image spinning task\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for x, y in loader_spinned:\n",
    "        x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        scores = model(x)\n",
    "        loss_auxillary = F.cross_entropy(scores[1], y[:, 1])\n",
    "        optimizer.zero_grad()\n",
    "        loss_auxillary.backward()\n",
    "        optimizer.step()\n",
    "    return evaluate_non_rotate(model, loader)  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Baseline ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.12248150981892375, val_acc:0.3569697742634868, val_loss improved from inf to 2.2720. Saving model to model_base_1.pth.\n",
      "[Epoch 1] train_acc: 0.6198355011476664, val_acc:0.7555158780767759, val_loss improved from 2.2720 to 0.8263. Saving model to model_base_1.pth.\n",
      "[Epoch 2] train_acc: 0.8285195103289977, val_acc:0.8477235046550184, val_loss improved from 0.8263 to 0.5327. Saving model to model_base_1.pth.\n",
      "[Epoch 3] train_acc: 0.8886444784493751, val_acc:0.884836117842112, val_loss improved from 0.5327 to 0.4087. Saving model to model_base_1.pth.\n",
      "[Epoch 4] train_acc: 0.9238714613618975, val_acc:0.8942736895804106, val_loss improved from 0.4087 to 0.3710. Saving model to model_base_1.pth.\n",
      "[Epoch 5] train_acc: 0.9361769956643713, val_acc:0.922203800535646, val_loss improved from 0.3710 to 0.2781. Saving model to model_base_1.pth.\n",
      "[Epoch 6] train_acc: 0.9510010201479214, val_acc:0.9234791480678485, val_loss improved from 0.2781 to 0.2671. Saving model to model_base_1.pth.\n",
      "[Epoch 7] train_acc: 0.961138740117317, val_acc:0.9233516133146282, val_loss did not improve from 0.2671\n",
      "[Epoch 8] train_acc: 0.962318286151492, val_acc:0.927305190664456, val_loss improved from 0.2869 to 0.2690. Saving model to model_base_1.pth.\n",
      "[Epoch 9] train_acc: 0.9696505993369039, val_acc:0.932789185052927, val_loss improved from 0.2690 to 0.2484. Saving model to model_base_1.pth.\n",
      "[Epoch 10] train_acc: 0.9769191532772252, val_acc:0.9307486290014029, val_loss did not improve from 0.2484\n",
      "[Epoch 11] train_acc: 0.9828168834480999, val_acc:0.9269225864047953, val_loss did not improve from 0.2612\n",
      "[Epoch 12] train_acc: 0.9821155317521041, val_acc:0.9480933554393572, val_loss improved from 0.2754 to 0.1953. Saving model to model_base_1.pth.\n",
      "[Epoch 13] train_acc: 0.9807447079826574, val_acc:0.9359775538834333, val_loss did not improve from 0.1953\n",
      "[Epoch 14] train_acc: 0.9850484570262689, val_acc:0.9341920673383497, val_loss did not improve from 0.2510\n",
      "[Epoch 15] train_acc: 0.9856222902320837, val_acc:0.9443948475959699, val_loss improved from 0.2590 to 0.2399. Saving model to model_base_1.pth.\n",
      "[Epoch 16] train_acc: 0.986259882682989, val_acc:0.9399311312332611, val_loss did not improve from 0.2399\n",
      "[Epoch 17] train_acc: 0.9894159653149707, val_acc:0.9395485269736003, val_loss did not improve from 0.2426\n",
      "[Epoch 18] train_acc: 0.981637337413925, val_acc:0.938910853207499, val_loss improved from 0.2651 to 0.2575. Saving model to model_base_1.pth.\n",
      "[Epoch 19] train_acc: 0.9880770211680694, val_acc:0.9438847085830889, val_loss improved from 0.2575 to 0.2490. Saving model to model_base_1.pth.\n",
      "[Epoch 20] train_acc: 0.9899897985207855, val_acc:0.9454151256217319, val_loss improved from 0.2490 to 0.2349. Saving model to model_base_1.pth.\n",
      "[Epoch 21] train_acc: 0.9938153532262178, val_acc:0.9307486290014029, val_loss did not improve from 0.2349\n",
      "[Epoch 22] train_acc: 0.9898304004080591, val_acc:0.9472006121668155, val_loss improved from 0.2999 to 0.2449. Saving model to model_base_1.pth.\n",
      "[Epoch 23] train_acc: 0.993241520020403, val_acc:0.9445223823491902, val_loss did not improve from 0.2449\n",
      "[Epoch 24] train_acc: 0.9894797245600612, val_acc:0.9487310292054585, val_loss improved from 0.2627 to 0.2421. Saving model to model_base_1.pth.\n",
      "[Epoch 25] train_acc: 0.9907230298393267, val_acc:0.9483484249457977, val_loss improved from 0.2421 to 0.2416. Saving model to model_base_1.pth.\n",
      "[Epoch 26] train_acc: 0.9912968630451415, val_acc:0.9341920673383497, val_loss did not improve from 0.2416\n",
      "[Epoch 27] train_acc: 0.9901810762560571, val_acc:0.9341920673383497, val_loss improved from 0.3036 to 0.2941. Saving model to model_base_1.pth.\n",
      "[Epoch 28] train_acc: 0.9902129558786024, val_acc:0.9472006121668155, val_loss improved from 0.2941 to 0.2269. Saving model to model_base_1.pth.\n",
      "[Epoch 29] train_acc: 0.9903085947462382, val_acc:0.9463078688942737, val_loss did not improve from 0.2269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<logger.Logger at 0x25be5ab5100>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "from models import BaselineResNet\n",
    "model_base_1 = BaselineResNet(43)\n",
    "optimizer = optim.Adam(model_base_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_base_1, optimizer, train_loader, val_loader, epochs=30, model_path='model_base_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Uncorrupted Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.43273189961910247, 0.9043547110055423)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import BaselineResNet\n",
    "model_path = 'model_base_1.pth'\n",
    "model_base_1 = BaselineResNet(43)\n",
    "params = torch.load(model_path)\n",
    "model_base_1.load_state_dict(params)\n",
    "model_base_1 = model_base_1.to(device=device)\n",
    "evaluate_main(model_base_1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1.5: Baseline CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.4020976791634787, val_acc:0.6983803086341028, val_loss improved from inf to 1.0278. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 1] train_acc: 0.8483167559296098, val_acc:0.8686392041831399, val_loss improved from 1.0278 to 0.4386. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 2] train_acc: 0.927250701351696, val_acc:0.907537303915317, val_loss improved from 0.4386 to 0.3340. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 3] train_acc: 0.9595766386125988, val_acc:0.9270501211580155, val_loss improved from 0.3340 to 0.2903. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 4] train_acc: 0.9738587095128793, val_acc:0.9296008162224206, val_loss did not improve from 0.2903\n",
      "[Epoch 5] train_acc: 0.9751657740372354, val_acc:0.9164647366407346, val_loss did not improve from 0.3384\n",
      "[Epoch 6] train_acc: 0.9775567457281306, val_acc:0.9380181099349573, val_loss improved from 0.3908 to 0.2848. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 7] train_acc: 0.9848890589135425, val_acc:0.936870297155975, val_loss did not improve from 0.2848\n",
      "[Epoch 8] train_acc: 0.9849209385360878, val_acc:0.938400714194618, val_loss did not improve from 0.3047\n",
      "[Epoch 9] train_acc: 0.9852078551389951, val_acc:0.9308761637546231, val_loss did not improve from 0.3289\n",
      "[Epoch 10] train_acc: 0.985303494006631, val_acc:0.9287080729498789, val_loss improved from 0.3562 to 0.3414. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 11] train_acc: 0.9865786789084418, val_acc:0.9441397780895294, val_loss improved from 0.3414 to 0.3043. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 12] train_acc: 0.9882364192807958, val_acc:0.9496237724780002, val_loss improved from 0.3043 to 0.2742. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 13] train_acc: 0.9929546034174955, val_acc:0.9400586659864814, val_loss did not improve from 0.2742\n",
      "[Epoch 14] train_acc: 0.9854628921193573, val_acc:0.9459252646346129, val_loss improved from 0.3621 to 0.2943. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 15] train_acc: 0.9888102524866106, val_acc:0.9470730774135953, val_loss did not improve from 0.2943\n",
      "[Epoch 16] train_acc: 0.9899579188982403, val_acc:0.9429919653105471, val_loss did not improve from 0.3051\n",
      "[Epoch 17] train_acc: 0.9920300943636827, val_acc:0.9473281469200358, val_loss improved from 0.3098 to 0.2800. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 18] train_acc: 0.9926995664371334, val_acc:0.9340645325851294, val_loss did not improve from 0.2800\n",
      "[Epoch 19] train_acc: 0.988618974751339, val_acc:0.9470730774135953, val_loss improved from 0.3917 to 0.2674. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 20] train_acc: 0.992061973986228, val_acc:0.9455426603749522, val_loss did not improve from 0.2674\n",
      "[Epoch 21] train_acc: 0.9914881407804131, val_acc:0.9492411682183395, val_loss improved from 0.2988 to 0.2788. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 22] train_acc: 0.9931140015302219, val_acc:0.9417166177783446, val_loss did not improve from 0.2788\n",
      "[Epoch 23] train_acc: 0.9913287426676868, val_acc:0.9493687029715597, val_loss improved from 0.3605 to 0.2722. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 24] train_acc: 0.9933052792654935, val_acc:0.9435021043234282, val_loss did not improve from 0.2722\n",
      "[Epoch 25] train_acc: 0.9895434838051518, val_acc:0.9418441525315648, val_loss improved from 0.3522 to 0.2924. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 26] train_acc: 0.9927952053047692, val_acc:0.9456701951281724, val_loss did not improve from 0.2924\n",
      "[Epoch 27] train_acc: 0.9911055853098699, val_acc:0.953577349827828, val_loss improved from 0.3265 to 0.2635. Saving model to cnn_model_base_1.pth.\n",
      "[Epoch 28] train_acc: 0.9956324917112981, val_acc:0.9477107511796965, val_loss did not improve from 0.2635\n",
      "[Epoch 29] train_acc: 0.9928589645498598, val_acc:0.937890575181737, val_loss did not improve from 0.3281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<logger.Logger at 0x25b0f00e0d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1: Train a baseline CNN: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "from cnn_models import BaselineCNN\n",
    "cnn_model_base_1 = BaselineCNN(43)\n",
    "optimizer = optim.Adam(cnn_model_base_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(cnn_model_base_1, optimizer, train_loader, val_loader, epochs=30, model_path='cnn_model_base_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "### Uncorrupted Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4685808004754962, 0.9095011876484561)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from cnn_models import BaselineCNN\n",
    "model_path = 'cnn_model_base_1.pth'\n",
    "cnn_model_base_1 = BaselineCNN(43)\n",
    "params = torch.load(model_path)\n",
    "cnn_model_base_1.load_state_dict(params)\n",
    "cnn_model_base_1 = cnn_model_base_1.to(device=device)\n",
    "evaluate_main(cnn_model_base_1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: ResNet18 with Auxillary Branch (No Online Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 0] train_acc: 0.07800146646263709, val_acc:0.11714067083280194, val_loss improved from inf to 3.2708. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 1] train_acc: 0.3623676995664371, val_acc:0.538387960719296, val_loss improved from 3.2708 to 1.4740. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 2] train_acc: 0.7030413159908186, val_acc:0.7750286953194746, val_loss improved from 1.4740 to 0.7251. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 3] train_acc: 0.8434551134914563, val_acc:0.8557900777961994, val_loss improved from 0.7251 to 0.4694. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 4] train_acc: 0.8951001020147922, val_acc:0.8846129320239765, val_loss improved from 0.4694 to 0.3755. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 5] train_acc: 0.9233135679673553, val_acc:0.8864621859456702, val_loss improved from 0.3755 to 0.3608. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 6] train_acc: 0.9367428589645499, val_acc:0.9079517918632828, val_loss improved from 0.3608 to 0.3081. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 7] train_acc: 0.9514553047691915, val_acc:0.915157505420227, val_loss improved from 0.3081 to 0.3055. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 8] train_acc: 0.9622306171894924, val_acc:0.9191110827700548, val_loss improved from 0.3055 to 0.2895. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 9] train_acc: 0.9614575363427696, val_acc:0.9103430684861625, val_loss did not improve from 0.2895\n",
      "[Epoch 10] train_acc: 0.9732450267788829, val_acc:0.9224269863537814, val_loss improved from 0.3129 to 0.2917. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 11] train_acc: 0.9764967482785004, val_acc:0.9255834714959826, val_loss improved from 0.2917 to 0.2754. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 12] train_acc: 0.9819561336393777, val_acc:0.933809463078689, val_loss improved from 0.2754 to 0.2704. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 13] train_acc: 0.9837732721244581, val_acc:0.9225864047953067, val_loss did not improve from 0.2704\n",
      "[Epoch 14] train_acc: 0.9838210915582759, val_acc:0.9177719678612422, val_loss did not improve from 0.3397\n",
      "[Epoch 15] train_acc: 0.9856222902320837, val_acc:0.9284211197551333, val_loss improved from 0.3418 to 0.3078. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 16] train_acc: 0.9839007906146391, val_acc:0.9333949751307231, val_loss improved from 0.3078 to 0.2924. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 17] train_acc: 0.9897108518235144, val_acc:0.930270373676827, val_loss did not improve from 0.2924\n",
      "[Epoch 18] train_acc: 0.9926517470033155, val_acc:0.9355949496237724, val_loss improved from 0.3158 to 0.3129. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 19] train_acc: 0.9859490563631726, val_acc:0.9286761892615738, val_loss improved from 0.3129 to 0.2909. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 20] train_acc: 0.9896789722009691, val_acc:0.9374442035454661, val_loss did not improve from 0.2909\n",
      "[Epoch 21] train_acc: 0.9940146008671258, val_acc:0.9342558347149599, val_loss did not improve from 0.2958\n",
      "[Epoch 22] train_acc: 0.9907867890844172, val_acc:0.9342239510266548, val_loss improved from 0.3162 to 0.3076. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 23] train_acc: 0.9894159653149707, val_acc:0.9354674148705522, val_loss did not improve from 0.3076\n",
      "[Epoch 24] train_acc: 0.9924923488905891, val_acc:0.9369659482208902, val_loss improved from 0.3092 to 0.3051. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 25] train_acc: 0.9912410737056874, val_acc:0.9360413212600434, val_loss improved from 0.3051 to 0.2974. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 26, Batch 100] train_loss: 0.041910067200660706"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-57fa8b6da310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexp_2_model_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetTwoBranch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_2_model_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_2_model_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rotate_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_rotate_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'exp_2_model_1.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-4d88fca17904>\u001b[0m in \u001b[0;36mtrain_both\u001b[0;34m(model, optimizer, loader_train, loader_val, epochs, model_path, early_stop_patience)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Experiment 1: Train a ResNet18 with auxillary branch\n",
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "from models import ResNetTwoBranch\n",
    "exp_2_model_1 = ResNetTwoBranch()\n",
    "optimizer = optim.Adam(exp_2_model_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_both(exp_2_model_1, optimizer, train_rotate_loader, val_rotate_loader, epochs=50, model_path='exp_2_model_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7415662362025335, 0.8653998416468726)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "from models import ResNetTwoBranch\n",
    "model_path = 'exp_2_model_1.pth'\n",
    "exp_2_model_1 = ResNetTwoBranch()\n",
    "params = torch.load(model_path)\n",
    "exp_2_model_1.load_state_dict(params)\n",
    "exp_2_model_1 = exp_2_model_1.to(device=device)\n",
    "evaluate_non_rotate(exp_2_model_1, test_loader)\n",
    "#evaluate_non_rotate(exp_2_model_1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2.5: CNN with Auxillary Branch (No Online Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 7.93 GiB total capacity; 5.64 GiB already allocated; 329.25 MiB free; 5.94 GiB reserved in total by PyTorch)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3c03c75c5720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexp_2_cnn_model_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNNTwoBranch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_2_cnn_model_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_2_cnn_model_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rotate_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_rotate_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'exp_2_cnn_model_1.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-4d88fca17904>\u001b[0m in \u001b[0;36mtrain_both\u001b[0;34m(model, optimizer, loader_train, loader_val, epochs, model_path, early_stop_patience)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ece176/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ece176/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 648.00 MiB (GPU 0; 7.93 GiB total capacity; 5.64 GiB already allocated; 329.25 MiB free; 5.94 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Experiment 1: Train a CNN with auxillary branch\n",
    "lr = 5e-4\n",
    "wd = 1e-5\n",
    "from cnn_models import CNNTwoBranch\n",
    "exp_2_cnn_model_1 = CNNTwoBranch()\n",
    "optimizer = optim.Adam(exp_2_cnn_model_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_both(exp_2_cnn_model_1, optimizer, train_rotate_loader, val_rotate_loader, epochs=50, model_path='exp_2_cnn_model_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.6484341713098379, 0.8699920823436262)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# Experiment 3: ResNet18 with Auxillary Branch (Online-Trained)\n",
    "lr = 1e-5\n",
    "# wd = 1e-8\n",
    "wd = 0\n",
    "from models import ResNetTwoBranch\n",
    "model_path = 'exp_2_model_1.pth'\n",
    "exp_3_model_1 = ResNetTwoBranch()\n",
    "params = torch.load(model_path)\n",
    "exp_3_model_1.load_state_dict(params)\n",
    "optimizer = optim.Adam(exp_3_model_1.parameters(), lr=lr, weight_decay=wd)\n",
    "ttt_online(exp_3_model_1, test_loader, test_rotate_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.7549277452322153, 0.8653998416468726)"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "from models import ResNetTwoBranch\n",
    "model_path = 'exp_2_model_1.pth'\n",
    "exp_3_model_1 = ResNetTwoBranch()\n",
    "params = torch.load(model_path)\n",
    "exp_3_model_1.load_state_dict(params)\n",
    "exp_3_model_1 = exp_3_model_1.to(device=device)\n",
    "evaluate_non_rotate(exp_3_model_1, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ResNetTwoBranch(\n  (resnet): BaselineResNet(\n    (feature_extractor): ResNet(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): Norm_Layer(\n        (group_norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n      )\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): Norm_Layer(\n            (group_norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n          )\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): Norm_Layer(\n            (group_norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): Norm_Layer(\n            (group_norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n          )\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): Norm_Layer(\n            (group_norm): GroupNorm(8, 64, eps=1e-05, affine=True)\n          )\n        )\n      )\n      (layer2): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): Norm_Layer(\n            (group_norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n          )\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): Norm_Layer(\n            (group_norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n          )\n          (downsample): Sequential(\n            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): Norm_Layer(\n              (group_norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n            )\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): Norm_Layer(\n            (group_norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n          )\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): Norm_Layer(\n            (group_norm): GroupNorm(8, 128, eps=1e-05, affine=True)\n          )\n        )\n      )\n      (layer3): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): Norm_Layer(\n            (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n          )\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): Norm_Layer(\n            (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n          )\n          (downsample): Sequential(\n            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): Norm_Layer(\n              (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n            )\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): Norm_Layer(\n            (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n          )\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): Norm_Layer(\n            (group_norm): GroupNorm(8, 256, eps=1e-05, affine=True)\n          )\n        )\n      )\n      (layer4): Sequential(\n        (0): BasicBlock(\n          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn1): Norm_Layer(\n            (group_norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n          )\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): Norm_Layer(\n            (group_norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n          )\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): Norm_Layer(\n              (group_norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n            )\n          )\n        )\n        (1): BasicBlock(\n          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn1): Norm_Layer(\n            (group_norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n          )\n          (relu): ReLU(inplace=True)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): Norm_Layer(\n            (group_norm): GroupNorm(8, 512, eps=1e-05, affine=True)\n          )\n        )\n      )\n      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n      (fc): Linear(in_features=512, out_features=1000, bias=True)\n    )\n  )\n  (relu): ReLU()\n  (fc_main): Linear(in_features=1000, out_features=43, bias=True)\n  (fc_auxillary): Linear(in_features=1000, out_features=4, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3: Do online training on the auxillary branch, with pre-trained shared and main branch weights from experiment 1.\n",
    "exp_3_model_1 = ResNetTwoBranch()\n",
    "print(exp_3_model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}