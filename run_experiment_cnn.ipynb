{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import sampler\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training pipelines\n",
    "def train_main(model, optimizer, loader_train, loader_val, epochs=1, model_path=None, early_stop_patience = 0):\n",
    "    \"\"\"\n",
    "    Train the main branch\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Logger object with loss and accuracy data\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    logger = Logger()\n",
    "    last_loss = float('inf')\n",
    "    for e in range(epochs):\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        total_loss = 0.0\n",
    "        count = 0\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"\\r[Epoch {e + 1}, Batch {t}] train_loss: {loss.item()}\", end='')\n",
    "            count += 1\n",
    "\n",
    "        # Conclude Epoch\n",
    "        train_loss = total_loss / count\n",
    "        train_acc = float(num_correct) / num_samples\n",
    "        val_loss, val_acc = evaluate_main(model, loader_val)\n",
    "        logger.log(train_loss, train_acc, val_loss, val_acc)\n",
    "        \n",
    "        with open(model_path.split('.')[0] + '.pkl', 'wb') as output_file:\n",
    "            pickle.dump(logger, output_file)\n",
    "\n",
    "        # Early Stopping\n",
    "        if logger.check_early_stop(early_stop_patience):\n",
    "            print(\"[Early Stopped]\")\n",
    "            break\n",
    "        else:\n",
    "            if last_loss > val_loss:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss improved from %.4f to %.4f. Saving model to {model_path}.\" % (last_loss, val_loss))\n",
    "                if model_path is not None:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss did not improve from %.4f\" % (last_loss))\n",
    "            last_loss = val_loss\n",
    "    return logger, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_both(model, optimizer, loader_train, loader_val, epochs=1, model_path=None, early_stop_patience = 0):\n",
    "    \"\"\"\n",
    "    Train the main and auxillary branch\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Logger object with loss and accuracy data\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    logger = Logger()\n",
    "    last_loss = float('inf')\n",
    "    for e in range(epochs):\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss_main = F.cross_entropy(scores[0], y[:, 0])\n",
    "            loss_auxillary = F.cross_entropy(scores[1], y[:, 1])\n",
    "            loss = loss_main + loss_auxillary\n",
    "            running_loss += loss_main.item()\n",
    "\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (preds == y[:, 0]).sum()\n",
    "            num_samples += preds.size(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\", end='')\n",
    "            count += 1\n",
    "\n",
    "        # Conclude Epoch\n",
    "        train_loss = running_loss / count\n",
    "        train_acc = float(num_correct) / num_samples\n",
    "        val_loss, val_acc = evaluate_both(model, loader_val)\n",
    "        logger.log(train_loss, train_acc, val_loss, val_acc)\n",
    "\n",
    "        with open(model_path.split('.')[0] + '.pkl', 'wb') as output_file:\n",
    "            pickle.dump(logger, output_file)\n",
    "            \n",
    "        # Early Stopping\n",
    "        if logger.check_early_stop(early_stop_patience):\n",
    "            print(\"[Early Stopped]\")\n",
    "            break\n",
    "        else:\n",
    "            if last_loss > val_loss:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss improved from %.4f to %.4f. Saving model to {model_path}.\" % (last_loss, val_loss))\n",
    "                if model_path is not None:\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "            else:\n",
    "                print(f\"\\r[Epoch {e}] train_acc: {train_acc}, val_acc:{val_acc}, val_loss did not improve from %.4f\" % (last_loss))\n",
    "            last_loss = val_loss\n",
    "    return logger, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_main(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    ave_loss = 0.0\n",
    "    count = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "            ave_loss += loss.item()\n",
    "            count += 1\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return ave_loss / count, acc\n",
    "\n",
    "def evaluate_both(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy in model with two predictions\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    ave_loss = 0.0\n",
    "    count = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss_main = F.cross_entropy(scores[0], y[:, 0])\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (preds == y[:, 0]).sum()\n",
    "            # print(f\"num_correct: {num_correct}\")\n",
    "            num_samples += preds.size(0)\n",
    "            # print(f\"num_samples: {num_samples}\")\n",
    "            ave_loss += loss_main.item()\n",
    "            count += 1\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return ave_loss / count, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_non_rotate(model, loader):\n",
    "    \"\"\"\n",
    "    Evaluate main branch accuracy in model with two predictions\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    from random import randrange\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    ave_loss = 0.0\n",
    "    count = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            loss_main = F.cross_entropy(scores[0], y)\n",
    "            # print(scores.shape)\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            #print(f\"num_correct: {num_correct}\")\n",
    "            num_samples += preds.size(0)\n",
    "            #print(f\"num_samples: {num_samples}\")\n",
    "            ave_loss += loss_main.item()\n",
    "            count += 1\n",
    "        acc = float(num_correct) / num_samples\n",
    "        return ave_loss / count, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def ttt(model, loader, loader_spinned, optimizer):\n",
    "    \"\"\"\n",
    "    TTT with image spinning task\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    original_model = copy.deepcopy(model.state_dict())\n",
    "    for x, y in loader_spinned:\n",
    "        x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        for i in range(4): # 4 step gradient descent\n",
    "            scores = model(x)\n",
    "            loss_auxillary = F.cross_entropy(scores[1], y[:, 1])\n",
    "            optimizer.zero_grad()\n",
    "            loss_auxillary.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = model(x)\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (y[preds == y[:, 0], 1] == 0).sum()\n",
    "            num_samples += (y[:, 1] == 0).sum()\n",
    "\n",
    "        model.load_state_dict(original_model)\n",
    "    acc = float(num_correct) / num_samples\n",
    "    return acc\n",
    "\n",
    "def ttt_online(model, loader, loader_spinned, optimizer):\n",
    "    \"\"\"\n",
    "    TTT online with image spinning task\n",
    "    Outputs: loss and accuracy\n",
    "    \"\"\"\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    for x, y in loader_spinned:\n",
    "        x = x.to(device=device, dtype=torch.float32)  # move to device, e.g. GPU\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "        scores = model(x)\n",
    "        loss_auxillary = F.cross_entropy(scores[1], y[:, 1])\n",
    "        optimizer.zero_grad()\n",
    "        loss_auxillary.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scores = model(x)\n",
    "            _, preds = scores[0].max(1)\n",
    "            num_correct += (y[preds == y[:, 0], 1] == 0).sum()\n",
    "            num_samples += (y[:, 1] == 0).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Baseline ResNet18\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.11511731701096659, val_acc:0.2923096543808188, val_loss improved from inf to 2.7940. Saving model to model_base_1.pth.\n",
      "[Epoch 1] train_acc: 0.4346467737821984, val_acc:0.6152276495344982, val_loss improved from 2.7940 to 1.4163. Saving model to model_base_1.pth.\n",
      "[Epoch 2] train_acc: 0.7363236419280795, val_acc:0.7913531437316669, val_loss improved from 1.4163 to 0.7206. Saving model to model_base_1.pth.\n",
      "[Epoch 3] train_acc: 0.8678589645498598, val_acc:0.871700038260426, val_loss improved from 0.7206 to 0.4909. Saving model to model_base_1.pth.\n",
      "[Epoch 4] train_acc: 0.9217674062739097, val_acc:0.8895549037112613, val_loss improved from 0.4909 to 0.4047. Saving model to model_base_1.pth.\n",
      "[Epoch 5] train_acc: 0.9497577148686559, val_acc:0.8922331335288867, val_loss improved from 0.4047 to 0.3886. Saving model to model_base_1.pth.\n",
      "[Epoch 6] train_acc: 0.9659844427441979, val_acc:0.9043489350848106, val_loss did not improve from 0.3886\n",
      "[Epoch 7] train_acc: 0.9736674317776077, val_acc:0.9122560897844663, val_loss improved from 0.3964 to 0.3559. Saving model to model_base_1.pth.\n",
      "[Epoch 8] train_acc: 0.9857179290997194, val_acc:0.9196531054712409, val_loss improved from 0.3559 to 0.3391. Saving model to model_base_1.pth.\n",
      "[Epoch 9] train_acc: 0.9922851313440448, val_acc:0.9228414743017472, val_loss did not improve from 0.3391\n",
      "[Epoch 10] train_acc: 0.9927633256822239, val_acc:0.9142966458359904, val_loss did not improve from 0.3470\n",
      "[Epoch 11] train_acc: 0.9908186687069626, val_acc:0.9149343196020916, val_loss improved from 0.3978 to 0.3910. Saving model to model_base_1.pth.\n",
      "[Epoch 12] train_acc: 0.9766003570517725, val_acc:0.910598137992603, val_loss improved from 0.3910 to 0.3883. Saving model to model_base_1.pth.\n",
      "[Epoch 13] train_acc: 0.9901491966335119, val_acc:0.9196531054712409, val_loss improved from 0.3883 to 0.3668. Saving model to model_base_1.pth.\n",
      "[Epoch 14] train_acc: 0.9939428717163988, val_acc:0.9260298431322536, val_loss improved from 0.3668 to 0.3625. Saving model to model_base_1.pth.\n",
      "[Epoch 15] train_acc: 0.9967163988778373, val_acc:0.9225864047953067, val_loss did not improve from 0.3625\n",
      "[Epoch 16] train_acc: 0.9960469268043867, val_acc:0.9176125494197169, val_loss improved from 0.4013 to 0.3984. Saving model to model_base_1.pth.\n",
      "[Epoch 17] train_acc: 0.9571219076766131, val_acc:0.9125111592909068, val_loss improved from 0.3984 to 0.3710. Saving model to model_base_1.pth.\n",
      "[Epoch 18] train_acc: 0.9911374649324152, val_acc:0.9284530034434383, val_loss improved from 0.3710 to 0.3476. Saving model to model_base_1.pth.\n",
      "[Epoch 19] train_acc: 0.9981828615149196, val_acc:0.9331717893125877, val_loss improved from 0.3476 to 0.3269. Saving model to model_base_1.pth.\n",
      "[Epoch 20] train_acc: 0.9991711298138231, val_acc:0.9353398801173319, val_loss did not improve from 0.3269\n",
      "[Epoch 21] train_acc: 0.9995855649069115, val_acc:0.9334268588190282, val_loss did not improve from 0.3310\n",
      "[Epoch 22] train_acc: 0.9997449630196379, val_acc:0.936360158143094, val_loss did not improve from 0.3355\n",
      "[Epoch 23] train_acc: 0.9998406018872736, val_acc:0.936870297155975, val_loss did not improve from 0.3444\n",
      "[Epoch 24] train_acc: 0.9997130833970925, val_acc:0.9390383879607193, val_loss improved from 0.3472 to 0.3365. Saving model to model_base_1.pth.\n",
      "[Epoch 25] train_acc: 0.9992667686814588, val_acc:0.9276877949241168, val_loss did not improve from 0.3365\n",
      "[Epoch 26] train_acc: 0.9988523335883703, val_acc:0.9296008162224206, val_loss improved from 0.3775 to 0.3660. Saving model to model_base_1.pth.\n",
      "[Epoch 27] train_acc: 0.9991711298138231, val_acc:0.9270501211580155, val_loss did not improve from 0.3660\n",
      "[Epoch 28] train_acc: 0.997385870951288, val_acc:0.924754495600051, val_loss improved from 0.3893 to 0.3852. Saving model to model_base_1.pth.\n",
      "[Epoch 29] train_acc: 0.9965251211425656, val_acc:0.9265399821451346, val_loss improved from 0.3852 to 0.3649. Saving model to model_base_1.pth.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<logger.Logger at 0x7fca6ad4de20>,\n",
       " ResNetMainBranch(\n",
       "   (resnet): BaselineResNet(\n",
       "     (feature_extractor): ResNet(\n",
       "       (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "       (bn1): Norm_Layer(\n",
       "         (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "       )\n",
       "       (relu): ReLU(inplace=True)\n",
       "       (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "       (layer1): Sequential(\n",
       "         (0): BasicBlock(\n",
       "           (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn1): Norm_Layer(\n",
       "             (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): Norm_Layer(\n",
       "             (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "           )\n",
       "         )\n",
       "         (1): BasicBlock(\n",
       "           (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn1): Norm_Layer(\n",
       "             (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): Norm_Layer(\n",
       "             (group_norm): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (layer2): Sequential(\n",
       "         (0): BasicBlock(\n",
       "           (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "           (bn1): Norm_Layer(\n",
       "             (group_norm): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): Norm_Layer(\n",
       "             (group_norm): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (downsample): Sequential(\n",
       "             (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "             (1): Norm_Layer(\n",
       "               (group_norm): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (1): BasicBlock(\n",
       "           (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn1): Norm_Layer(\n",
       "             (group_norm): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): Norm_Layer(\n",
       "             (group_norm): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (layer3): Sequential(\n",
       "         (0): BasicBlock(\n",
       "           (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "           (bn1): Norm_Layer(\n",
       "             (group_norm): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): Norm_Layer(\n",
       "             (group_norm): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (downsample): Sequential(\n",
       "             (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "             (1): Norm_Layer(\n",
       "               (group_norm): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (1): BasicBlock(\n",
       "           (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn1): Norm_Layer(\n",
       "             (group_norm): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): Norm_Layer(\n",
       "             (group_norm): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (layer4): Sequential(\n",
       "         (0): BasicBlock(\n",
       "           (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "           (bn1): Norm_Layer(\n",
       "             (group_norm): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): Norm_Layer(\n",
       "             (group_norm): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (downsample): Sequential(\n",
       "             (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "             (1): Norm_Layer(\n",
       "               (group_norm): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
       "             )\n",
       "           )\n",
       "         )\n",
       "         (1): BasicBlock(\n",
       "           (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn1): Norm_Layer(\n",
       "             (group_norm): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
       "           )\n",
       "           (relu): ReLU(inplace=True)\n",
       "           (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "           (bn2): Norm_Layer(\n",
       "             (group_norm): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "       (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "       (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "     )\n",
       "   )\n",
       "   (relu): ReLU()\n",
       "   (fc): Linear(in_features=1000, out_features=43, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "batch_size = 1024\n",
    "train_set = TensorDataset(torch.load('train_x.pt'), torch.load('train_y.pt'))\n",
    "val_set = TensorDataset(torch.load('val_x.pt'), torch.load('val_y.pt'))\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from models import ResNetMainBranch\n",
    "model_base_1 = ResNetMainBranch()\n",
    "optimizer = optim.Adam(model_base_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_base_1, optimizer, train_loader, val_loader, epochs=30, model_path='model_base_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Uncorrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrupted test set:  (0.6300153686450078, 0.8663499604117181)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "batch_size = 1024\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "from models import ResNetMainBranch\n",
    "model_path = 'model_base_1.pth'\n",
    "model_base_1 = ResNetMainBranch()\n",
    "params = torch.load(model_path)\n",
    "model_base_1.load_state_dict(params)\n",
    "model_base_1 = model_base_1.to(device=device)\n",
    "print('Uncorrupted test set: ', evaluate_main(model_base_1, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Corrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise:  (2.487208182995136, 0.6048297703879651)\n",
      "Defocus blur:  (0.704764301960285, 0.8528107680126682)\n",
      "Elastic transform:  (0.8109714067899264, 0.8272367379255741)\n",
      "Horizontal Motion blur:  (1.66626422221844, 0.6737133808392716)\n",
      "Virtical Motion blur:  (1.7649270937992976, 0.6558986539984165)\n",
      "Zoom blur:  (0.6961561670670142, 0.8586698337292161)\n",
      "Shot:  (1.4280506464151235, 0.7303246239113222)\n",
      "Impulse:  (4.0637488548572245, 0.4505146476642914)\n",
      "Contrast:  (2.1393172099040103, 0.6636579572446556)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "test_gauss_set = TensorDataset(torch.load('test_x_gauss_noise.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_gauss_loader = DataLoader(test_gauss_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_defocus_set = TensorDataset(torch.load('test_x_defocus_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_defocus_loader = DataLoader(test_defocus_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_elastic_set = TensorDataset(torch.load('test_x_elastic_transform.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_elastic_loader = DataLoader(test_elastic_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_hmotion_set = TensorDataset(torch.load('test_x_hmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_hmotion_loader = DataLoader(test_hmotion_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_vmotion_set = TensorDataset(torch.load('test_x_vmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_vmotion_loader = DataLoader(test_vmotion_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_zoom_set = TensorDataset(torch.load('test_x_zoom_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_zoom_loader = DataLoader(test_zoom_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_shot_set = TensorDataset(torch.load('test_x_shot.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_shot_loader = DataLoader(test_shot_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_impulse_set = TensorDataset(torch.load('test_x_impulse.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_impulse_loader = DataLoader(test_impulse_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_contrast_set = TensorDataset(torch.load('test_x_contrast.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_contrast_loader = DataLoader(test_contrast_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from models import ResNetMainBranch\n",
    "model_path = 'model_base_1.pth'\n",
    "model_base_1 = ResNetMainBranch()\n",
    "params = torch.load(model_path)\n",
    "model_base_1.load_state_dict(params)\n",
    "model_base_1 = model_base_1.to(device=device)\n",
    "\n",
    "print(\"Gaussian noise: \", evaluate_main(model_base_1, test_gauss_loader))\n",
    "print(\"Defocus blur: \", evaluate_main(model_base_1, test_defocus_loader))\n",
    "print(\"Elastic transform: \", evaluate_main(model_base_1, test_elastic_loader))\n",
    "print(\"Horizontal Motion blur: \", evaluate_main(model_base_1, test_hmotion_loader))\n",
    "print(\"Virtical Motion blur: \", evaluate_main(model_base_1, test_vmotion_loader))\n",
    "print(\"Zoom blur: \", evaluate_main(model_base_1, test_zoom_loader))\n",
    "print(\"Shot: \", evaluate_main(model_base_1, test_shot_loader))\n",
    "print(\"Impulse: \", evaluate_main(model_base_1, test_impulse_loader))\n",
    "print(\"Contrast: \", evaluate_main(model_base_1, test_contrast_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1.5: Baseline CNN\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.44478449375159396, val_acc:0.7945415125621732, val_loss improved from inf to 0.6778. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 1] train_acc: 0.872991583779648, val_acc:0.8996301492156613, val_loss improved from 0.6778 to 0.3613. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 2] train_acc: 0.926900025503698, val_acc:0.9284530034434383, val_loss improved from 0.3613 to 0.2514. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 3] train_acc: 0.9473029839326702, val_acc:0.9120010202780258, val_loss did not improve from 0.2514\n",
      "[Epoch 4] train_acc: 0.9590665646518746, val_acc:0.9174850146664966, val_loss did not improve from 0.3109\n",
      "[Epoch 5] train_acc: 0.9663988778372864, val_acc:0.9377630404285168, val_loss improved from 0.3446 to 0.2477. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 6] train_acc: 0.9742731446059678, val_acc:0.93431960209157, val_loss did not improve from 0.2477\n",
      "[Epoch 7] train_acc: 0.9751976536597806, val_acc:0.9396760617268206, val_loss improved from 0.2761 to 0.2383. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 8] train_acc: 0.9811910226982913, val_acc:0.9426093610508863, val_loss did not improve from 0.2383\n",
      "[Epoch 9] train_acc: 0.9775567457281306, val_acc:0.9409514092590231, val_loss improved from 0.2708 to 0.2674. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 10] train_acc: 0.9837095128793675, val_acc:0.9409514092590231, val_loss did not improve from 0.2674\n",
      "[Epoch 11] train_acc: 0.9828487630706453, val_acc:0.9302384899885219, val_loss did not improve from 0.3542\n",
      "[Epoch 12] train_acc: 0.9834544758990054, val_acc:0.9423542915444458, val_loss improved from 0.3622 to 0.2805. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 13] train_acc: 0.9860686049477174, val_acc:0.947965820686137, val_loss improved from 0.2805 to 0.2354. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 14] train_acc: 0.989097169089518, val_acc:0.9459252646346129, val_loss did not improve from 0.2354\n",
      "[Epoch 15] train_acc: 0.9857179290997194, val_acc:0.9493687029715597, val_loss improved from 0.2691 to 0.2422. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 16] train_acc: 0.9850803366488141, val_acc:0.9464354036474939, val_loss did not improve from 0.2422\n",
      "[Epoch 17] train_acc: 0.9871843917368018, val_acc:0.9526846065552863, val_loss did not improve from 0.2427\n",
      "[Epoch 18] train_acc: 0.9862280030604438, val_acc:0.9429919653105471, val_loss did not improve from 0.2587\n",
      "[Epoch 19] train_acc: 0.9901810762560571, val_acc:0.9487310292054585, val_loss improved from 0.2948 to 0.2573. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 20] train_acc: 0.9900854373884214, val_acc:0.9493687029715597, val_loss did not improve from 0.2573\n",
      "[Epoch 21] train_acc: 0.9871525121142566, val_acc:0.9510266547634231, val_loss did not improve from 0.2647\n",
      "[Epoch 22] train_acc: 0.9891928079571538, val_acc:0.937380436168856, val_loss did not improve from 0.2656\n",
      "[Epoch 23] train_acc: 0.9935603162458556, val_acc:0.9475832164264762, val_loss improved from 0.3868 to 0.2418. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 24] train_acc: 0.9890015302218822, val_acc:0.9427368958041066, val_loss did not improve from 0.2418\n",
      "[Epoch 25] train_acc: 0.9881089007906146, val_acc:0.9420992220380053, val_loss did not improve from 0.2845\n",
      "[Epoch 26] train_acc: 0.9893840856924254, val_acc:0.9477107511796965, val_loss improved from 0.3389 to 0.2334. Saving model to model_cnn_base_1.pth.\n",
      "[Epoch 27] train_acc: 0.9923170109665902, val_acc:0.9486034944522382, val_loss did not improve from 0.2334\n",
      "[Epoch 28] train_acc: 0.9905317521040551, val_acc:0.9446499171024104, val_loss did not improve from 0.2593\n",
      "[Epoch 29] train_acc: 0.9916475388931395, val_acc:0.9442673128427497, val_loss improved from 0.3176 to 0.2732. Saving model to model_cnn_base_1.pth.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<logger.Logger at 0x14b18a35a30>,\n",
       " CNNMainBranch(\n",
       "   (cnn): BaselineCNN(\n",
       "     (conv1): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "     (maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (fc1): Linear(in_features=18432, out_features=9216, bias=True)\n",
       "     (fc2): Linear(in_features=9216, out_features=1000, bias=True)\n",
       "   )\n",
       "   (relu): ReLU()\n",
       "   (fc): Linear(in_features=1000, out_features=43, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1: Train a baseline ResNet18: no branch\n",
    "lr = 1e-3\n",
    "wd = 1e-4\n",
    "batch_size = 64\n",
    "train_set = TensorDataset(torch.load('train_x.pt'), torch.load('train_y.pt'))\n",
    "val_set = TensorDataset(torch.load('val_x.pt'), torch.load('val_y.pt'))\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from cnn_models import CNNMainBranch\n",
    "model_cnn_base_1 = CNNMainBranch()\n",
    "optimizer = optim.Adam(model_cnn_base_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_main(model_cnn_base_1, optimizer, train_loader, val_loader, epochs=30, model_path='model_cnn_base_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Uncorrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrupted test set:  (0.4815320502119986, 0.9054631828978622)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "batch_size = 64\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from cnn_models import CNNMainBranch\n",
    "model_path = 'model_cnn_base_1.pth'\n",
    "model_cnn_base_1 = CNNMainBranch()\n",
    "params = torch.load(model_path)\n",
    "model_cnn_base_1.load_state_dict(params)\n",
    "model_cnn_base_1 = model_cnn_base_1.to(device=device)\n",
    "\n",
    "print('Uncorrupted test set: ', evaluate_main(model_cnn_base_1, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Corrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise:  (1.044238771001498, 0.7676959619952494)\n",
      "Defocus blur:  (0.49437062746158456, 0.8997624703087886)\n",
      "Elastic transform:  (0.6439499969434257, 0.8684877276326207)\n",
      "Horizontal Motion blur:  (1.075326208213363, 0.7423594615993666)\n",
      "Virtical Motion blur:  (1.9190652045336636, 0.5697545526524149)\n",
      "Zoom blur:  (0.5435288520247648, 0.8913697545526524)\n",
      "Shot:  (0.6516357510529384, 0.8585114806017419)\n",
      "Impulse:  (1.182600476224013, 0.729612034837688)\n",
      "Contrast:  (1.5937491890155908, 0.6174980205859065)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "test_gauss_set = TensorDataset(torch.load('test_x_gauss_noise.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_gauss_loader = DataLoader(test_gauss_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_defocus_set = TensorDataset(torch.load('test_x_defocus_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_defocus_loader = DataLoader(test_defocus_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_elastic_set = TensorDataset(torch.load('test_x_elastic_transform.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_elastic_loader = DataLoader(test_elastic_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_hmotion_set = TensorDataset(torch.load('test_x_hmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_hmotion_loader = DataLoader(test_hmotion_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_vmotion_set = TensorDataset(torch.load('test_x_vmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_vmotion_loader = DataLoader(test_vmotion_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_zoom_set = TensorDataset(torch.load('test_x_zoom_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_zoom_loader = DataLoader(test_zoom_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_shot_set = TensorDataset(torch.load('test_x_shot.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_shot_loader = DataLoader(test_shot_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_impulse_set = TensorDataset(torch.load('test_x_impulse.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_impulse_loader = DataLoader(test_impulse_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_contrast_set = TensorDataset(torch.load('test_x_contrast.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_contrast_loader = DataLoader(test_contrast_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from cnn_models import CNNMainBranch\n",
    "model_path = 'model_cnn_base_1.pth'\n",
    "model_cnn_base_1 = CNNMainBranch()\n",
    "params = torch.load(model_path)\n",
    "model_cnn_base_1.load_state_dict(params)\n",
    "model_cnn_base_1 = model_cnn_base_1.to(device=device)\n",
    "\n",
    "print(\"Gaussian noise: \", evaluate_main(model_cnn_base_1, test_gauss_loader))\n",
    "print(\"Defocus blur: \", evaluate_main(model_cnn_base_1, test_defocus_loader))\n",
    "print(\"Elastic transform: \", evaluate_main(model_cnn_base_1, test_elastic_loader))\n",
    "print(\"Horizontal Motion blur: \", evaluate_main(model_cnn_base_1, test_hmotion_loader))\n",
    "print(\"Virtical Motion blur: \", evaluate_main(model_cnn_base_1, test_vmotion_loader))\n",
    "print(\"Zoom blur: \", evaluate_main(model_cnn_base_1, test_zoom_loader))\n",
    "print(\"Shot: \", evaluate_main(model_cnn_base_1, test_shot_loader))\n",
    "print(\"Impulse: \", evaluate_main(model_cnn_base_1, test_impulse_loader))\n",
    "print(\"Contrast: \", evaluate_main(model_cnn_base_1, test_contrast_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on the traditional enhanced set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "test_enhanced_set = TensorDataset(torch.load('test_x_enhanced_impulse.pt'), torch.load('test_y_enhanced_impulse.pt'))\n",
    "test_enhanced_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from cnn_models import CNNMainBranch\n",
    "model_path = 'model_cnn_base_1.pth'\n",
    "model_cnn_base_1 = CNNMainBranch()\n",
    "params = torch.load(model_path)\n",
    "model_cnn_base_1.load_state_dict(params)\n",
    "model_cnn_base_1 = model_cnn_base_1.to(device=device)\n",
    "\n",
    "print('Uncorrupted test set: ', evaluate_main(model_cnn_base_1, test_enhanced_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: ResNet18 with Auxillary Branch (No Online Training)\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train_acc: 0.22435284366233105, val_acc:0.5029014156357607, val_loss improved from inf to 1.6574. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 1] train_acc: 0.6622353991328742, val_acc:0.7657824257110063, val_loss improved from 1.6574 to 0.7710. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 2] train_acc: 0.8420205304769192, val_acc:0.8594567019512818, val_loss improved from 0.7710 to 0.4682. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 3] train_acc: 0.8975707727620506, val_acc:0.8927751562300726, val_loss improved from 0.4682 to 0.3605. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 4] train_acc: 0.927402129558786, val_acc:0.8969200357097309, val_loss improved from 0.3605 to 0.3528. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 5] train_acc: 0.9433339709257842, val_acc:0.906038770564979, val_loss improved from 0.3528 to 0.3133. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 6] train_acc: 0.9564683754144351, val_acc:0.9068358627726055, val_loss did not improve from 0.3133\n",
      "[Epoch 7] train_acc: 0.9628602397347615, val_acc:0.9153488075500574, val_loss improved from 0.3328 to 0.3023. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 8] train_acc: 0.9702642820709003, val_acc:0.9209284530034434, val_loss did not improve from 0.3023\n",
      "[Epoch 9] train_acc: 0.9743608135679673, val_acc:0.9202907792373421, val_loss improved from 0.3070 to 0.3070. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 10] train_acc: 0.9779313312930374, val_acc:0.9223951026654763, val_loss did not improve from 0.3070\n",
      "[Epoch 11] train_acc: 0.9791507268553941, val_acc:0.9246269608468307, val_loss improved from 0.3201 to 0.3165. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 12] train_acc: 0.9840442489160929, val_acc:0.9215661267695447, val_loss did not improve from 0.3165\n",
      "[Epoch 13] train_acc: 0.9860048457026269, val_acc:0.926795051651575, val_loss improved from 0.3501 to 0.3296. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 14] train_acc: 0.9873916092833461, val_acc:0.9280066318071675, val_loss improved from 0.3296 to 0.3237. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 15] train_acc: 0.9858693573068095, val_acc:0.9286443055732687, val_loss did not improve from 0.3237\n",
      "[Epoch 16] train_acc: 0.9880770211680694, val_acc:0.9255515878076775, val_loss did not improve from 0.3249\n",
      "[Epoch 17] train_acc: 0.9866583779648049, val_acc:0.9215342430812397, val_loss improved from 0.3640 to 0.3561. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 18] train_acc: 0.9883958173935221, val_acc:0.9335862772605535, val_loss improved from 0.3561 to 0.3046. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 19] train_acc: 0.9930821219076766, val_acc:0.9327254176763168, val_loss did not improve from 0.3046\n",
      "[Epoch 20] train_acc: 0.9906353608773272, val_acc:0.9269225864047953, val_loss did not improve from 0.3380\n",
      "[Epoch 21] train_acc: 0.9885870951287937, val_acc:0.9285805381966586, val_loss improved from 0.3621 to 0.3434. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 22] train_acc: 0.9932733996429483, val_acc:0.9352442290524168, val_loss improved from 0.3434 to 0.3312. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 23] train_acc: 0.990579571537873, val_acc:0.9339688815202143, val_loss did not improve from 0.3312\n",
      "[Epoch 24] train_acc: 0.991583779648049, val_acc:0.9341920673383497, val_loss did not improve from 0.3347\n",
      "[Epoch 25] train_acc: 0.9914961106860495, val_acc:0.932279046040046, val_loss did not improve from 0.3373\n",
      "[Epoch 26] train_acc: 0.9925321984187707, val_acc:0.9347022063512307, val_loss improved from 0.3611 to 0.3367. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 27] train_acc: 0.992364830400408, val_acc:0.9347978574161458, val_loss did not improve from 0.3367\n",
      "[Epoch 28] train_acc: 0.9915120504973222, val_acc:0.9264124473919143, val_loss did not improve from 0.3395\n",
      "[Epoch 29] train_acc: 0.9921337031369548, val_acc:0.9362645070781788, val_loss improved from 0.3749 to 0.3262. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 30] train_acc: 0.991583779648049, val_acc:0.9292500956510649, val_loss did not improve from 0.3262\n",
      "[Epoch 31] train_acc: 0.992564078041316, val_acc:0.9374442035454661, val_loss improved from 0.3601 to 0.3279. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 32] train_acc: 0.9932973093598572, val_acc:0.9360094375717383, val_loss improved from 0.3279 to 0.3265. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 33] train_acc: 0.9933849783218567, val_acc:0.9332355566891978, val_loss did not improve from 0.3265\n",
      "[Epoch 34] train_acc: 0.992364830400408, val_acc:0.9348935084810611, val_loss did not improve from 0.3389\n",
      "[Epoch 35] train_acc: 0.993066182096404, val_acc:0.9379543425583472, val_loss improved from 0.3441 to 0.3254. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 36] train_acc: 0.992866934455496, val_acc:0.9362645070781788, val_loss improved from 0.3254 to 0.3247. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 37] train_acc: 0.9928988140780414, val_acc:0.9352123453641117, val_loss did not improve from 0.3247\n",
      "[Epoch 38] train_acc: 0.993217610303494, val_acc:0.9391978064022446, val_loss improved from 0.3461 to 0.3229. Saving model to exp_2_model_1.pth.\n",
      "[Epoch 39] train_acc: 0.9939030221882172, val_acc:0.9379543425583472, val_loss did not improve from 0.3229\n",
      "[Epoch 40] train_acc: 0.99422181841367, val_acc:0.9371891340390256, val_loss did not improve from 0.3284\n",
      "[Epoch 41, Batch 85] train_loss: 0.02040305733680725"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-51db1bbc2d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mexp_2_model_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNetTwoBranch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_2_model_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtrain_both\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_2_model_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_rotate_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_rotate_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'exp_2_model_1.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-24a675d3695d>\u001b[0m in \u001b[0;36mtrain_both\u001b[0;34m(model, optimizer, loader_train, loader_val, epochs, model_path, early_stop_patience)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\r[Epoch {e}, Batch {t}] train_loss: {loss.item()}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "batch_size = 1024\n",
    "train_mean = [86.69585, 86.342995, 85.84817]\n",
    "train_std = [74.59906, 74.196365, 73.890495]\n",
    "\n",
    "train_rotate_set = TensorDataset(torch.load('train_x_rotate.pt'), torch.load('train_y_rotate.pt'))\n",
    "val_rotate_set = TensorDataset(torch.load('val_x_rotate.pt'), torch.load('val_y_rotate.pt'))\n",
    "\n",
    "train_rotate_loader = DataLoader(train_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "val_rotate_loader = DataLoader(val_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from models import ResNetTwoBranch\n",
    "exp_2_model_1 = ResNetTwoBranch()\n",
    "optimizer = optim.Adam(exp_2_model_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_both(exp_2_model_1, optimizer, train_rotate_loader, val_rotate_loader, epochs=50, model_path='exp_2_model_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Uncorrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrupted test set:  (0.7577824179942791, 0.8707046714172605)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from models import ResNetTwoBranch\n",
    "model_path = 'exp_2_model_1.pth'\n",
    "exp_2_model_1 = ResNetTwoBranch()\n",
    "params = torch.load(model_path)\n",
    "exp_2_model_1.load_state_dict(params)\n",
    "exp_2_model_1 = exp_2_model_1.to(device=device)\n",
    "print('Uncorrupted test set: ', evaluate_non_rotate(exp_2_model_1, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Corrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise:  (3.3912662175985484, 0.6321456848772763)\n",
      "Defocus blur:  (0.8509451242593619, 0.8607284243863816)\n",
      "Elastic transform:  (1.0640129309434156, 0.8300870942201108)\n",
      "Horizontal Motion blur:  (2.1406559577355018, 0.6624703087885986)\n",
      "Virtical Motion blur:  (2.1507714803402243, 0.6610451306413302)\n",
      "Zoom blur:  (0.8463354981862582, 0.8596991290577989)\n",
      "Shot:  (1.79858844096844, 0.7488519398258116)\n",
      "Impulse:  (5.357336667867807, 0.49382422802850356)\n",
      "Contrast:  (2.2237631724430966, 0.7022961203483769)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "test_gauss_set = TensorDataset(torch.load('test_x_gauss_noise.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_gauss_loader = DataLoader(test_gauss_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_defocus_set = TensorDataset(torch.load('test_x_defocus_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_defocus_loader = DataLoader(test_defocus_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_elastic_set = TensorDataset(torch.load('test_x_elastic_transform.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_elastic_loader = DataLoader(test_elastic_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_hmotion_set = TensorDataset(torch.load('test_x_hmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_hmotion_loader = DataLoader(test_hmotion_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_vmotion_set = TensorDataset(torch.load('test_x_vmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_vmotion_loader = DataLoader(test_vmotion_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_zoom_set = TensorDataset(torch.load('test_x_zoom_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_zoom_loader = DataLoader(test_zoom_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_shot_set = TensorDataset(torch.load('test_x_shot.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_shot_loader = DataLoader(test_shot_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_impulse_set = TensorDataset(torch.load('test_x_impulse.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_impulse_loader = DataLoader(test_impulse_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_contrast_set = TensorDataset(torch.load('test_x_contrast.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_contrast_loader = DataLoader(test_contrast_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from models import ResNetTwoBranch\n",
    "model_path = 'exp_2_model_1.pth'\n",
    "exp_2_model_1 = ResNetTwoBranch()\n",
    "params = torch.load(model_path)\n",
    "exp_2_model_1.load_state_dict(params)\n",
    "exp_2_model_1 = exp_2_model_1.to(device=device)\n",
    "\n",
    "print(\"Gaussian noise: \", evaluate_non_rotate(exp_2_model_1, test_gauss_loader))\n",
    "print(\"Defocus blur: \", evaluate_non_rotate(exp_2_model_1, test_defocus_loader))\n",
    "print(\"Elastic transform: \", evaluate_non_rotate(exp_2_model_1, test_elastic_loader))\n",
    "print(\"Horizontal Motion blur: \", evaluate_non_rotate(exp_2_model_1, test_hmotion_loader))\n",
    "print(\"Vertical Motion blur: \", evaluate_non_rotate(exp_2_model_1, test_vmotion_loader))\n",
    "print(\"Zoom blur: \", evaluate_non_rotate(exp_2_model_1, test_zoom_loader))\n",
    "print(\"Shot: \", evaluate_non_rotate(exp_2_model_1, test_shot_loader))\n",
    "print(\"Impulse: \", evaluate_non_rotate(exp_2_model_1, test_impulse_loader))\n",
    "print(\"Contrast: \", evaluate_non_rotate(exp_2_model_1, test_contrast_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2.5: CNN with Auxillary Branch (No Online Training)\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-5\n",
    "batch_size = 64\n",
    "train_mean = [86.69585, 86.342995, 85.84817]\n",
    "train_std = [74.59906, 74.196365, 73.890495]\n",
    "\n",
    "train_rotate_set = TensorDataset(torch.load('train_x_rotate.pt'), torch.load('train_y_rotate.pt'))\n",
    "val_rotate_set = TensorDataset(torch.load('val_x_rotate.pt'), torch.load('val_y_rotate.pt'))\n",
    "\n",
    "train_rotate_loader = DataLoader(train_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "val_rotate_loader = DataLoader(val_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from cnn_models import CNNTwoBranch\n",
    "exp_2_cnn_model_1 = CNNTwoBranch()\n",
    "optimizer = optim.Adam(exp_2_cnn_model_1.parameters(), lr=lr, weight_decay=wd)\n",
    "train_both(exp_2_cnn_model_1, optimizer, train_rotate_loader, val_rotate_loader, epochs=30, model_path='exp_2_cnn_model_1.pth', early_stop_patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Uncorrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrupted test set:  (0.5389380103936701, 0.9046714172604909)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from cnn_models import CNNTwoBranch\n",
    "model_path = 'exp_2_cnn_model_1.pth'\n",
    "exp_2_cnn_model_1 = CNNTwoBranch()\n",
    "params = torch.load(model_path)\n",
    "exp_2_cnn_model_1.load_state_dict(params)\n",
    "exp_2_cnn_model_1 = exp_2_cnn_model_1.to(device=device)\n",
    "print('Uncorrupted test set: ', evaluate_non_rotate(exp_2_cnn_model_1, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Corrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise:  (1.5474033264013438, 0.7116389548693587)\n",
      "Defocus blur:  (0.5702346425790054, 0.8922406967537608)\n",
      "Elastic transform:  (0.7285343683682955, 0.8669041963578781)\n",
      "Horizontal Motion blur:  (1.6213888205014741, 0.6400633412509897)\n",
      "Vertical Motion blur:  (2.0637162832113414, 0.5583531274742676)\n",
      "Zoom blur:  (0.591289235995366, 0.8904196357878068)\n",
      "Shot:  (0.8383319698847257, 0.8400633412509897)\n",
      "Impulse:  (2.1770995396834154, 0.5878859857482185)\n",
      "Contrast:  (3.185165441953219, 0.4818685669041964)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "test_gauss_set = TensorDataset(torch.load('test_x_gauss_noise.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_gauss_loader = DataLoader(test_gauss_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_defocus_set = TensorDataset(torch.load('test_x_defocus_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_defocus_loader = DataLoader(test_defocus_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_elastic_set = TensorDataset(torch.load('test_x_elastic_transform.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_elastic_loader = DataLoader(test_elastic_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_hmotion_set = TensorDataset(torch.load('test_x_hmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_hmotion_loader = DataLoader(test_hmotion_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_vmotion_set = TensorDataset(torch.load('test_x_vmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_vmotion_loader = DataLoader(test_vmotion_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_zoom_set = TensorDataset(torch.load('test_x_zoom_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_zoom_loader = DataLoader(test_zoom_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_shot_set = TensorDataset(torch.load('test_x_shot.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_shot_loader = DataLoader(test_shot_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_impulse_set = TensorDataset(torch.load('test_x_impulse.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_impulse_loader = DataLoader(test_impulse_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_contrast_set = TensorDataset(torch.load('test_x_contrast.pt'), torch.load('test_y_corrupted.pt'))\n",
    "test_contrast_loader = DataLoader(test_contrast_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "from cnn_models import CNNTwoBranch\n",
    "model_path = 'exp_2_cnn_model_1.pth'\n",
    "exp_2_cnn_model_1 = CNNTwoBranch()\n",
    "params = torch.load(model_path)\n",
    "exp_2_cnn_model_1.load_state_dict(params)\n",
    "exp_2_cnn_model_1 = exp_2_cnn_model_1.to(device=device)\n",
    "\n",
    "print(\"Gaussian noise: \", evaluate_non_rotate(exp_2_cnn_model_1, test_gauss_loader))\n",
    "print(\"Defocus blur: \", evaluate_non_rotate(exp_2_cnn_model_1, test_defocus_loader))\n",
    "print(\"Elastic transform: \", evaluate_non_rotate(exp_2_cnn_model_1, test_elastic_loader))\n",
    "print(\"Horizontal Motion blur: \", evaluate_non_rotate(exp_2_cnn_model_1, test_hmotion_loader))\n",
    "print(\"Vertical Motion blur: \", evaluate_non_rotate(exp_2_cnn_model_1, test_vmotion_loader))\n",
    "print(\"Zoom blur: \", evaluate_non_rotate(exp_2_cnn_model_1, test_zoom_loader))\n",
    "print(\"Shot: \", evaluate_non_rotate(exp_2_cnn_model_1, test_shot_loader))\n",
    "print(\"Impulse: \", evaluate_non_rotate(exp_2_cnn_model_1, test_impulse_loader))\n",
    "print(\"Contrast: \", evaluate_non_rotate(exp_2_cnn_model_1, test_contrast_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: ResNet18 with Auxillary Branch (TTT)\n",
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_part3(model_path):\n",
    "    lr = 1e-3\n",
    "    from models import ResNetTwoBranch\n",
    "    exp_3_model_1 = ResNetTwoBranch()\n",
    "    params = torch.load(model_path)\n",
    "    exp_3_model_1.load_state_dict(params)\n",
    "    optimizer = optim.SGD(exp_3_model_1.parameters(), lr=lr)\n",
    "    return exp_3_model_1, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTT on Uncorrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrupted test set:  tensor(0.8705, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_rotate_set = TensorDataset(torch.load('test_x_rotate.pt'), torch.load('test_y_rotate.pt'))\n",
    "test_rotate_loader = DataLoader(test_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "exp_3_model_1, optimizer = initialize_model_part3('exp_2_model_1.pth')\n",
    "print('Uncorrupted test set: ', ttt(exp_3_model_1, test_loader, test_rotate_loader, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTT on Corrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise :  tensor(0.6324, device='cuda:0')\n",
      "Defocus blur :  tensor(0.8609, device='cuda:0')\n",
      "Elastic transform :  tensor(0.8310, device='cuda:0')\n",
      "Horizontal motion blur :  tensor(0.6643, device='cuda:0')\n",
      "Vertical motion blur :  tensor(0.6676, device='cuda:0')\n",
      "Zoom blur :  tensor(0.8603, device='cuda:0')\n",
      "Shot :  tensor(0.7460, device='cuda:0')\n",
      "Impulse :  tensor(0.4981, device='cuda:0')\n",
      "Contrast :  tensor(0.7095, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "\n",
    "def gauss():\n",
    "    test_gauss_set = TensorDataset(torch.load('test_x_gauss_noise.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_gauss_loader = DataLoader(test_gauss_set, batch_size=batch_size, shuffle=True)\n",
    "    test_gauss_rotate_set = TensorDataset(torch.load('test_x_rotate_gauss_noise.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_gauss_rotate_loader = DataLoader(test_gauss_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_gauss_loader, test_gauss_rotate_loader\n",
    "\n",
    "def defocus():\n",
    "    test_defocus_set = TensorDataset(torch.load('test_x_defocus_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_defocus_loader = DataLoader(test_defocus_set, batch_size=batch_size, shuffle=True)\n",
    "    test_defocus_rotate_set = TensorDataset(torch.load('test_x_rotate_defocus_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_defocus_rotate_loader = DataLoader(test_defocus_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_defocus_loader, test_defocus_rotate_loader\n",
    "\n",
    "def elastic():\n",
    "    test_elastic_set = TensorDataset(torch.load('test_x_elastic_transform.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_elastic_loader = DataLoader(test_elastic_set, batch_size=batch_size, shuffle=True)\n",
    "    test_elastic_rotate_set = TensorDataset(torch.load('test_x_rotate_elastic_transform.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_elastic_rotate_loader = DataLoader(test_elastic_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_elastic_loader, test_elastic_rotate_loader\n",
    "\n",
    "def hmotion():\n",
    "    test_hmotion_set = TensorDataset(torch.load('test_x_hmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_hmotion_loader = DataLoader(test_hmotion_set, batch_size=batch_size, shuffle=True)\n",
    "    test_hmotion_rotate_set = TensorDataset(torch.load('test_x_rotate_hmotion_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_hmotion_rotate_loader = DataLoader(test_hmotion_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_hmotion_loader, test_hmotion_rotate_loader\n",
    "\n",
    "def vmotion():\n",
    "    test_vmotion_set = TensorDataset(torch.load('test_x_vmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_vmotion_loader = DataLoader(test_vmotion_set, batch_size=batch_size, shuffle=True)\n",
    "    test_vmotion_rotate_set = TensorDataset(torch.load('test_x_rotate_vmotion_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_vmotion_rotate_loader = DataLoader(test_vmotion_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_vmotion_loader, test_vmotion_rotate_loader\n",
    "\n",
    "def zoom():\n",
    "    test_zoom_set = TensorDataset(torch.load('test_x_zoom_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_zoom_loader = DataLoader(test_zoom_set, batch_size=batch_size, shuffle=True)\n",
    "    test_zoom_rotate_set = TensorDataset(torch.load('test_x_rotate_zoom_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_zoom_rotate_loader = DataLoader(test_zoom_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_zoom_loader, test_zoom_rotate_loader\n",
    "\n",
    "def shot():\n",
    "    test_shot_set = TensorDataset(torch.load('test_x_shot.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_shot_loader = DataLoader(test_shot_set, batch_size=batch_size, shuffle=True)\n",
    "    test_shot_rotate_set = TensorDataset(torch.load('test_x_rotate_shot.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_shot_rotate_loader = DataLoader(test_shot_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_shot_loader, test_shot_rotate_loader\n",
    "\n",
    "def impulse():\n",
    "    test_impulse_set = TensorDataset(torch.load('test_x_impulse.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_impulse_loader = DataLoader(test_impulse_set, batch_size=batch_size, shuffle=True)\n",
    "    test_impulse_rotate_set = TensorDataset(torch.load('test_x_rotate_impulse.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_impulse_rotate_loader = DataLoader(test_impulse_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_impulse_loader,  test_impulse_rotate_loader\n",
    "\n",
    "def contrast():     \n",
    "    test_contrast_set = TensorDataset(torch.load('test_x_contrast.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_contrast_loader = DataLoader(test_contrast_set, batch_size=batch_size, shuffle=True)\n",
    "    test_contrast_rotate_set = TensorDataset(torch.load('test_x_rotate_contrast.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_contrast_rotate_loader = DataLoader(test_contrast_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_contrast_loader, test_contrast_rotate_loader\n",
    "\n",
    "test_loaders = [gauss, defocus, elastic, hmotion, vmotion, zoom, shot, impulse, contrast]\n",
    "test_names = [\"Gaussian noise\", \"Defocus blur\", \"Elastic transform\", 'Horizontal motion blur', 'Vertical motion blur', 'Zoom blur', 'Shot', 'Impulse', 'Contrast']\n",
    "\n",
    "for i in range(len(test_loaders)):\n",
    "    exp_3_model_1, optimizer = initialize_model_part3('exp_2_model_1.pth')\n",
    "    loaders = test_loaders[i]()\n",
    "    print(test_names[i], ': ', ttt(exp_3_model_1, loaders[0], loaders[1], optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3.5: CNN with Auxillary Branch (TTT)\n",
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_cnn_model_part3(model_path):\n",
    "    lr = 1e-3\n",
    "    from cnn_models import CNNTwoBranch\n",
    "    exp_3_cnn_model_1 = CNNTwoBranch()\n",
    "    params = torch.load(model_path)\n",
    "    exp_3_cnn_model_1.load_state_dict(params)\n",
    "    optimizer = optim.SGD(exp_3_cnn_model_1.parameters(), lr=lr)\n",
    "    return exp_3_cnn_model_1, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTT on Uncorrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrupted test set:  tensor(0.9046, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_rotate_set = TensorDataset(torch.load('test_x_rotate.pt'), torch.load('test_y_rotate.pt'))\n",
    "test_rotate_loader = DataLoader(test_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "exp_3_cnn_model_1, optimizer = initialize_cnn_model_part3('exp_2_cnn_model_1.pth')\n",
    "print('Uncorrupted test set: ', ttt(exp_3_cnn_model_1, test_loader, test_rotate_loader, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTT on Corrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise :  tensor(0.7134, device='cuda:0')\n",
      "Defocus blur :  tensor(0.8931, device='cuda:0')\n",
      "Elastic transform :  tensor(0.8660, device='cuda:0')\n",
      "Horizontal motion blur :  tensor(0.6478, device='cuda:0')\n",
      "Vertical motion blur :  tensor(0.5719, device='cuda:0')\n",
      "Zoom blur :  tensor(0.8918, device='cuda:0')\n",
      "Shot :  tensor(0.8429, device='cuda:0')\n",
      "Impulse :  tensor(0.5901, device='cuda:0')\n",
      "Contrast :  tensor(0.5160, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def gauss():\n",
    "    test_gauss_set = TensorDataset(torch.load('test_x_gauss_noise.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_gauss_loader = DataLoader(test_gauss_set, batch_size=batch_size, shuffle=True)\n",
    "    test_gauss_rotate_set = TensorDataset(torch.load('test_x_rotate_gauss_noise.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_gauss_rotate_loader = DataLoader(test_gauss_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_gauss_loader, test_gauss_rotate_loader\n",
    "\n",
    "def defocus():\n",
    "    test_defocus_set = TensorDataset(torch.load('test_x_defocus_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_defocus_loader = DataLoader(test_defocus_set, batch_size=batch_size, shuffle=True)\n",
    "    test_defocus_rotate_set = TensorDataset(torch.load('test_x_rotate_defocus_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_defocus_rotate_loader = DataLoader(test_defocus_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_defocus_loader, test_defocus_rotate_loader\n",
    "\n",
    "def elastic():\n",
    "    test_elastic_set = TensorDataset(torch.load('test_x_elastic_transform.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_elastic_loader = DataLoader(test_elastic_set, batch_size=batch_size, shuffle=True)\n",
    "    test_elastic_rotate_set = TensorDataset(torch.load('test_x_rotate_elastic_transform.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_elastic_rotate_loader = DataLoader(test_elastic_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_elastic_loader, test_elastic_rotate_loader\n",
    "\n",
    "def hmotion():\n",
    "    test_hmotion_set = TensorDataset(torch.load('test_x_hmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_hmotion_loader = DataLoader(test_hmotion_set, batch_size=batch_size, shuffle=True)\n",
    "    test_hmotion_rotate_set = TensorDataset(torch.load('test_x_rotate_hmotion_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_hmotion_rotate_loader = DataLoader(test_hmotion_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_hmotion_loader, test_hmotion_rotate_loader\n",
    "\n",
    "def vmotion():\n",
    "    test_vmotion_set = TensorDataset(torch.load('test_x_vmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_vmotion_loader = DataLoader(test_vmotion_set, batch_size=batch_size, shuffle=True)\n",
    "    test_vmotion_rotate_set = TensorDataset(torch.load('test_x_rotate_vmotion_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_vmotion_rotate_loader = DataLoader(test_vmotion_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_vmotion_loader, test_vmotion_rotate_loader\n",
    "\n",
    "def zoom():\n",
    "    test_zoom_set = TensorDataset(torch.load('test_x_zoom_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_zoom_loader = DataLoader(test_zoom_set, batch_size=batch_size, shuffle=True)\n",
    "    test_zoom_rotate_set = TensorDataset(torch.load('test_x_rotate_zoom_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_zoom_rotate_loader = DataLoader(test_zoom_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_zoom_loader, test_zoom_rotate_loader\n",
    "\n",
    "def shot():\n",
    "    test_shot_set = TensorDataset(torch.load('test_x_shot.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_shot_loader = DataLoader(test_shot_set, batch_size=batch_size, shuffle=True)\n",
    "    test_shot_rotate_set = TensorDataset(torch.load('test_x_rotate_shot.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_shot_rotate_loader = DataLoader(test_shot_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_shot_loader, test_shot_rotate_loader\n",
    "\n",
    "def impulse():\n",
    "    test_impulse_set = TensorDataset(torch.load('test_x_impulse.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_impulse_loader = DataLoader(test_impulse_set, batch_size=batch_size, shuffle=True)\n",
    "    test_impulse_rotate_set = TensorDataset(torch.load('test_x_rotate_impulse.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_impulse_rotate_loader = DataLoader(test_impulse_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_impulse_loader,  test_impulse_rotate_loader\n",
    "\n",
    "def contrast():     \n",
    "    test_contrast_set = TensorDataset(torch.load('test_x_contrast.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_contrast_loader = DataLoader(test_contrast_set, batch_size=batch_size, shuffle=True)\n",
    "    test_contrast_rotate_set = TensorDataset(torch.load('test_x_rotate_contrast.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_contrast_rotate_loader = DataLoader(test_contrast_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "    return test_contrast_loader, test_contrast_rotate_loader\n",
    "\n",
    "test_loaders = [gauss, defocus, elastic, hmotion, vmotion, zoom, shot, impulse, contrast]\n",
    "test_names = [\"Gaussian noise\", \"Defocus blur\", \"Elastic transform\", 'Horizontal motion blur', 'Vertical motion blur', 'Zoom blur', 'Shot', 'Impulse', 'Contrast']\n",
    "\n",
    "for i in range(len(test_loaders)):\n",
    "    exp_3_cnn_model_1, optimizer = initialize_cnn_model_part3('exp_2_cnn_model_1.pth')\n",
    "    loaders = test_loaders[i]()\n",
    "    print(test_names[i], ': ', ttt(exp_3_cnn_model_1, loaders[0], loaders[1], optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4: ResNet18 with Auxillary Branch (TTT Online)\n",
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_part4(model_path):\n",
    "    lr = 1e-3\n",
    "    wd = 0\n",
    "    from models import ResNetTwoBranch\n",
    "    exp_4_model_1 = ResNetTwoBranch()\n",
    "    params = torch.load(model_path)\n",
    "    exp_4_model_1.load_state_dict(params)\n",
    "    optimizer = optim.SGD(exp_4_model_1.parameters(), lr=lr)\n",
    "    return exp_4_model_1, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online TTT on Uncorrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrupted test set:  tensor(0.8674, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "test_rotate_set = TensorDataset(torch.load('test_x_rotate.pt'), torch.load('test_y_rotate.pt'))\n",
    "test_rotate_loader = DataLoader(test_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "exp_3_model_1, optimizer = initialize_model_part4('exp_2_model_1.pth')\n",
    "print('Uncorrupted test set: ', ttt_online(exp_3_model_1, test_loader, test_rotate_loader, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online TTT on Corrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'initialize_model_part3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7ac6fcbfe9b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mexp_3_model_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_model_part3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exp_2_model_1.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mttt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_3_model_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'initialize_model_part3' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "def gauss():\n",
    "    test_gauss_set = TensorDataset(torch.load('test_x_gauss_noise.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_gauss_loader = DataLoader(test_gauss_set, batch_size=batch_size, shuffle=False)\n",
    "    test_gauss_rotate_set = TensorDataset(torch.load('test_x_rotate_gauss_noise.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_gauss_rotate_loader = DataLoader(test_gauss_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_gauss_loader, test_gauss_rotate_loader\n",
    "\n",
    "def defocus():\n",
    "    test_defocus_set = TensorDataset(torch.load('test_x_defocus_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_defocus_loader = DataLoader(test_defocus_set, batch_size=batch_size, shuffle=False)\n",
    "    test_defocus_rotate_set = TensorDataset(torch.load('test_x_rotate_defocus_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_defocus_rotate_loader = DataLoader(test_defocus_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_defocus_loader, test_defocus_rotate_loader\n",
    "\n",
    "def elastic():\n",
    "    test_elastic_set = TensorDataset(torch.load('test_x_elastic_transform.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_elastic_loader = DataLoader(test_elastic_set, batch_size=batch_size, shuffle=False)\n",
    "    test_elastic_rotate_set = TensorDataset(torch.load('test_x_rotate_elastic_transform.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_elastic_rotate_loader = DataLoader(test_elastic_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_elastic_loader, test_elastic_rotate_loader\n",
    "\n",
    "def hmotion():\n",
    "    test_hmotion_set = TensorDataset(torch.load('test_x_hmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_hmotion_loader = DataLoader(test_hmotion_set, batch_size=batch_size, shuffle=False)\n",
    "    test_hmotion_rotate_set = TensorDataset(torch.load('test_x_rotate_hmotion_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_hmotion_rotate_loader = DataLoader(test_hmotion_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_hmotion_loader, test_hmotion_rotate_loader\n",
    "\n",
    "def vmotion():\n",
    "    test_vmotion_set = TensorDataset(torch.load('test_x_vmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_vmotion_loader = DataLoader(test_vmotion_set, batch_size=batch_size, shuffle=False)\n",
    "    test_vmotion_rotate_set = TensorDataset(torch.load('test_x_rotate_vmotion_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_vmotion_rotate_loader = DataLoader(test_vmotion_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_vmotion_loader, test_vmotion_rotate_loader\n",
    "\n",
    "def zoom():\n",
    "    test_zoom_set = TensorDataset(torch.load('test_x_zoom_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_zoom_loader = DataLoader(test_zoom_set, batch_size=batch_size, shuffle=False)\n",
    "    test_zoom_rotate_set = TensorDataset(torch.load('test_x_rotate_zoom_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_zoom_rotate_loader = DataLoader(test_zoom_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_zoom_loader, test_zoom_rotate_loader\n",
    "\n",
    "def shot():\n",
    "    test_shot_set = TensorDataset(torch.load('test_x_shot.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_shot_loader = DataLoader(test_shot_set, batch_size=batch_size, shuffle=False)\n",
    "    test_shot_rotate_set = TensorDataset(torch.load('test_x_rotate_shot.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_shot_rotate_loader = DataLoader(test_shot_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_shot_loader, test_shot_rotate_loader\n",
    "\n",
    "def impulse():\n",
    "    test_impulse_set = TensorDataset(torch.load('test_x_impulse.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_impulse_loader = DataLoader(test_impulse_set, batch_size=batch_size, shuffle=False)\n",
    "    test_impulse_rotate_set = TensorDataset(torch.load('test_x_rotate_impulse.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_impulse_rotate_loader = DataLoader(test_impulse_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_impulse_loader,  test_impulse_rotate_loader\n",
    "\n",
    "def contrast():     \n",
    "    test_contrast_set = TensorDataset(torch.load('test_x_contrast.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_contrast_loader = DataLoader(test_contrast_set, batch_size=batch_size, shuffle=False)\n",
    "    test_contrast_rotate_set = TensorDataset(torch.load('test_x_rotate_contrast.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_contrast_rotate_loader = DataLoader(test_contrast_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_contrast_loader, test_contrast_rotate_loader\n",
    "\n",
    "test_loaders = [gauss, defocus, elastic, hmotion, vmotion, zoom, shot, impulse, contrast]\n",
    "test_names = [\"Gaussian noise\", \"Defocus blur\", \"Elastic transform\", 'Horizontal motion blur', 'Vertical motion blur', 'Zoom blur', 'Shot', 'Impulse', 'Contrast']\n",
    "\n",
    "for i in range(len(test_loaders)):\n",
    "    exp_3_model_1, optimizer = initialize_model_part4('exp_2_model_1.pth')\n",
    "    loaders = test_loaders[i]()\n",
    "    print(test_names[i], ': ', ttt(exp_3_model_1, loaders[0], loaders[1], optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4.5: CNN with Auxillary Branch (TTT Online)\n",
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_cnn_model_part4(model_path):\n",
    "    lr = 1e-3\n",
    "    wd = 0\n",
    "    from cnn_models import CNNTwoBranch\n",
    "    exp_4_cnn_model_1 = CNNTwoBranch()\n",
    "    params = torch.load(model_path)\n",
    "    exp_4_cnn_model_1.load_state_dict(params)\n",
    "    optimizer = optim.SGD(exp_4_cnn_model_1.parameters(), lr=lr)\n",
    "    return exp_4_cnn_model_1, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online TTT on Uncorrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrupted test set:  tensor(0.8971, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "test_set = TensorDataset(torch.load('test_x.pt'), torch.load('test_y.pt'))\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "test_rotate_set = TensorDataset(torch.load('test_x_rotate.pt'), torch.load('test_y_rotate.pt'))\n",
    "test_rotate_loader = DataLoader(test_rotate_set, batch_size=batch_size, shuffle=True)\n",
    "exp_4_cnn_model_1, optimizer = initialize_cnn_model_part4('exp_2_cnn_model_1.pth')\n",
    "print('Uncorrupted test set: ', ttt_online(exp_4_cnn_model_1, test_loader, test_rotate_loader, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online TTT on Corrupted Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian noise :  tensor(0.7277, device='cuda:0')\n",
      "Defocus blur :  tensor(0.9004, device='cuda:0')\n",
      "Elastic transform :  tensor(0.8735, device='cuda:0')\n",
      "Horizontal motion blur :  tensor(0.6664, device='cuda:0')\n",
      "Vertical motion blur :  tensor(0.6032, device='cuda:0')\n",
      "Zoom blur :  tensor(0.8956, device='cuda:0')\n",
      "Shot :  tensor(0.8541, device='cuda:0')\n",
      "Impulse :  tensor(0.6165, device='cuda:0')\n",
      "Contrast :  tensor(0.5667, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "\n",
    "def gauss():\n",
    "    test_gauss_set = TensorDataset(torch.load('test_x_gauss_noise.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_gauss_loader = DataLoader(test_gauss_set, batch_size=batch_size, shuffle=False)\n",
    "    test_gauss_rotate_set = TensorDataset(torch.load('test_x_rotate_gauss_noise.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_gauss_rotate_loader = DataLoader(test_gauss_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_gauss_loader, test_gauss_rotate_loader\n",
    "\n",
    "def defocus():\n",
    "    test_defocus_set = TensorDataset(torch.load('test_x_defocus_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_defocus_loader = DataLoader(test_defocus_set, batch_size=batch_size, shuffle=False)\n",
    "    test_defocus_rotate_set = TensorDataset(torch.load('test_x_rotate_defocus_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_defocus_rotate_loader = DataLoader(test_defocus_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_defocus_loader, test_defocus_rotate_loader\n",
    "\n",
    "def elastic():\n",
    "    test_elastic_set = TensorDataset(torch.load('test_x_elastic_transform.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_elastic_loader = DataLoader(test_elastic_set, batch_size=batch_size, shuffle=False)\n",
    "    test_elastic_rotate_set = TensorDataset(torch.load('test_x_rotate_elastic_transform.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_elastic_rotate_loader = DataLoader(test_elastic_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_elastic_loader, test_elastic_rotate_loader\n",
    "\n",
    "def hmotion():\n",
    "    test_hmotion_set = TensorDataset(torch.load('test_x_hmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_hmotion_loader = DataLoader(test_hmotion_set, batch_size=batch_size, shuffle=False)\n",
    "    test_hmotion_rotate_set = TensorDataset(torch.load('test_x_rotate_hmotion_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_hmotion_rotate_loader = DataLoader(test_hmotion_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_hmotion_loader, test_hmotion_rotate_loader\n",
    "\n",
    "def vmotion():\n",
    "    test_vmotion_set = TensorDataset(torch.load('test_x_vmotion_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_vmotion_loader = DataLoader(test_vmotion_set, batch_size=batch_size, shuffle=False)\n",
    "    test_vmotion_rotate_set = TensorDataset(torch.load('test_x_rotate_vmotion_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_vmotion_rotate_loader = DataLoader(test_vmotion_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_vmotion_loader, test_vmotion_rotate_loader\n",
    "\n",
    "def zoom():\n",
    "    test_zoom_set = TensorDataset(torch.load('test_x_zoom_blur.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_zoom_loader = DataLoader(test_zoom_set, batch_size=batch_size, shuffle=False)\n",
    "    test_zoom_rotate_set = TensorDataset(torch.load('test_x_rotate_zoom_blur.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_zoom_rotate_loader = DataLoader(test_zoom_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_zoom_loader, test_zoom_rotate_loader\n",
    "\n",
    "def shot():\n",
    "    test_shot_set = TensorDataset(torch.load('test_x_shot.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_shot_loader = DataLoader(test_shot_set, batch_size=batch_size, shuffle=False)\n",
    "    test_shot_rotate_set = TensorDataset(torch.load('test_x_rotate_shot.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_shot_rotate_loader = DataLoader(test_shot_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_shot_loader, test_shot_rotate_loader\n",
    "\n",
    "def impulse():\n",
    "    test_impulse_set = TensorDataset(torch.load('test_x_impulse.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_impulse_loader = DataLoader(test_impulse_set, batch_size=batch_size, shuffle=False)\n",
    "    test_impulse_rotate_set = TensorDataset(torch.load('test_x_rotate_impulse.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_impulse_rotate_loader = DataLoader(test_impulse_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_impulse_loader,  test_impulse_rotate_loader\n",
    "\n",
    "def contrast():     \n",
    "    test_contrast_set = TensorDataset(torch.load('test_x_contrast.pt'), torch.load('test_y_corrupted.pt'))\n",
    "    test_contrast_loader = DataLoader(test_contrast_set, batch_size=batch_size, shuffle=False)\n",
    "    test_contrast_rotate_set = TensorDataset(torch.load('test_x_rotate_contrast.pt'), torch.load('test_y_rotate_corrupted.pt'))\n",
    "    test_contrast_rotate_loader = DataLoader(test_contrast_rotate_set, batch_size=batch_size, shuffle=False)\n",
    "    return test_contrast_loader, test_contrast_rotate_loader\n",
    "\n",
    "test_loaders = [gauss, defocus, elastic, hmotion, vmotion, zoom, shot, impulse, contrast]\n",
    "test_names = [\"Gaussian noise\", \"Defocus blur\", \"Elastic transform\", 'Horizontal motion blur', 'Vertical motion blur', 'Zoom blur', 'Shot', 'Impulse', 'Contrast']\n",
    "\n",
    "for i in range(len(test_loaders)):\n",
    "    exp_4_cnn_model_1, optimizer = initialize_cnn_model_part4('exp_2_cnn_model_1.pth')\n",
    "    loaders = test_loaders[i]()\n",
    "    print(test_names[i], ': ', ttt(exp_4_cnn_model_1, loaders[0], loaders[1], optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
